{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# Tensorflow with GPU\n",
        "\n",
        "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oM_8ELnJq_wd"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXnDmXR7RDr2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7da56af5-3fba-45ec-a11c-4f240549d39d"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v3fE7KmKRDsH"
      },
      "source": [
        "## Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "This example constructs a typical convolutional neural network layer over a\n",
        "random image and manually places the resulting ops on either the CPU or the GPU\n",
        "to compare execution speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y04m-jvKRDsJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "9de3364f-06cd-4f89-8dc2-15672a944d01"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.151080382998771\n",
            "GPU (s):\n",
            "0.11110841200024879\n",
            "GPU speedup over CPU: 28x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsloTr4qtuSr",
        "colab_type": "text"
      },
      "source": [
        "## To access files in google drive we have to connect google colab to google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "beAtFJzEsJMb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ceb3d23d-d8bb-4f19-d3e5-07cabd5b1e1c"
      },
      "source": [
        "from google.colab import drive #For using the google drive for data\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ef7zVK1vsMUZ",
        "colab": {}
      },
      "source": [
        "import numpy as np#Array handling\n",
        "import pandas as pd#Working with dataframes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58j-QbGJtuTE",
        "colab_type": "text"
      },
      "source": [
        "### Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6KS-b9zjsPeX",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv('/content/drive/My Drive/dataset/train.csv')\n",
        "test=pd.read_csv('/content/drive/My Drive/dataset/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T9Uu0-TrsPiD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "e2d770e7-ba17-401a-94f6-5f507d4c5330"
      },
      "source": [
        "train.head()#Make sure data is read correctly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>96.jpg</td>\n",
              "      <td>manipuri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>163.jpg</td>\n",
              "      <td>bharatanatyam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>450.jpg</td>\n",
              "      <td>odissi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>219.jpg</td>\n",
              "      <td>kathakali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>455.jpg</td>\n",
              "      <td>odissi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Image         target\n",
              "0   96.jpg       manipuri\n",
              "1  163.jpg  bharatanatyam\n",
              "2  450.jpg         odissi\n",
              "3  219.jpg      kathakali\n",
              "4  455.jpg         odissi"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N1mng-QIsPlT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "062860ed-2ea0-4e6e-a7ab-b051a5055a5f"
      },
      "source": [
        "test.head()#See data is correctly read"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>508.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>246.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>473.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>485.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Image\n",
              "0  508.jpg\n",
              "1  246.jpg\n",
              "2  473.jpg\n",
              "3  485.jpg\n",
              "4  128.jpg"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A9AKHYVusxP8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d3924461-f9a0-40ca-bd19-1361d0078779"
      },
      "source": [
        "train['target'].unique() #Check all the dance forms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['manipuri', 'bharatanatyam', 'odissi', 'kathakali', 'kathak',\n",
              "       'sattriya', 'kuchipudi', 'mohiniyattam'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78vbJ6nVtuTe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e8aaedb5-f342-4e0b-b5cf-6a888415973c"
      },
      "source": [
        "'''\n",
        "We cannot train a model on categorical label so we have to convert it into numerical\n",
        "class_map --> it is used to conmvert categorical to numerical to perform computation\n",
        "inverse_map --> it is used to convert the numerical output obtained after computation into categorical form\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\nWe cannot train a model on categorical label so we have to convert it into numerical\\nclass_map --> it is used to conmvert categorical to numerical to perform computation\\ninverse_map --> it is used to convert the numerical output obtained after computation into categorical form\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dVI4r8n5sxSh",
        "colab": {}
      },
      "source": [
        "class_map={'manipuri':0,'bharatanatyam':1,'odissi':2, 'kathakali':3, 'kathak':4,'sattriya':5, 'kuchipudi':6, 'mohiniyattam':7}\n",
        "inverse_map={0:'manipuri',1:'bharatanatyam',2:'odissi',3:'kathakali',4: 'kathak',5:'sattriya',6:'kuchipudi',7:'mohiniyattam'}\n",
        "train['target']=train['target'].map(class_map) #maps the two series of class_map[dance forms,ints] dance_forms->ints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MA4PxwWTsxW6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "4e52c981-9744-4f9f-826a-a0e8aaabf11e"
      },
      "source": [
        "train.head() #Check above mapping worked or not"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>96.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>163.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>450.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>219.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>455.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Image  target\n",
              "0   96.jpg       0\n",
              "1  163.jpg       1\n",
              "2  450.jpg       2\n",
              "3  219.jpg       3\n",
              "4  455.jpg       2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUnmnq-YtuTx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7ddecd47-0792-47f2-93f5-e35439082b6f"
      },
      "source": [
        "'''\n",
        "List of pretrained Models for image classification are:\n",
        "1) Xception\n",
        "2) VGG16\n",
        "3) VGG19\n",
        "4) ResNet50\n",
        "5) InceptionV3\n",
        "6) InceptionResNetV2\n",
        "7) MobileNet\n",
        "8) MobileNetV2\n",
        "9) DenseNet\n",
        "10) NASNet\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\nList of pretrained Models for image classification are:\\n1) Xception\\n2) VGG16\\n3) VGG19\\n4) ResNet50\\n5) InceptionV3\\n6) InceptionResNetV2\\n7) MobileNet\\n8) MobileNetV2\\n9) DenseNet\\n10) NASNet\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho8ba0PWtuT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5f10aaab-2d70-4eb9-dc69-f3350021f5ee"
      },
      "source": [
        "'''\n",
        "All the pretrained models listed above have their weights trained on ImageNet Data\n",
        "All the pretrained models have the shape of image as (224,224,3) \n",
        "so to perform transfer learning using above pretrained models we have to convert out images to (224,224,3)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAll the pretrained models listed above have their weights trained on ImageNet Data\\nAll the pretrained models have the shape of image as (224,224,3) \\nso to perform transfer learning using above pretrained models we have to convert out images to (224,224,3)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uh4gvtw2sxaJ",
        "colab": {}
      },
      "source": [
        "img_h,img_w=(224,224) #For using the standard models to make use of transfer learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FBHDwio0sxfj",
        "colab": {}
      },
      "source": [
        "import os#This module provides a portable way of using operating system dependent functionality.\n",
        "import seaborn as sns #For graphs\n",
        "import cv2 #it is used for computer vision applications like image processing,video capture analysis and like face and object detection\n",
        "from tqdm import tqdm #Shows the progess bar of how much a loop is executed or a pipeline is executed.\n",
        "import matplotlib.pyplot as plt #For plotting."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ct5ZQQCtuUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4af69da3-4d01-44ae-a12e-ab89a7f12a5e"
      },
      "source": [
        "'''\n",
        "1) we have to read the images from the given path\n",
        "2) resize the image into standard shape\n",
        "3) and converting the image array into floaating point array\n",
        "4) and append each array into train_img\n",
        "5) append each output label into train_label\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\n1) we have to read the images from the given path\\n2) resize the image into standard shape\\n3) and converting the image array into floaating point array\\n4) and append each array into train_img\\n5) append each output label into train_label\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_77_NT-4sxkx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "209d6682-b034-46e2-c66d-5793d4b6c2b3"
      },
      "source": [
        "train_img=[] #In this list we will have images of required type as we needed\n",
        "train_label=[] #lables for images \n",
        "j=0\n",
        "path='/content/drive/My Drive/dataset/train'\n",
        "for i in tqdm(train['Image']):#using tqdm we can get a progress bar showing how much its done\n",
        "    final_path=os.path.join(path,i) #setting up path\n",
        "    img=cv2.imread(final_path) #reading the image\n",
        "    img=cv2.resize(img,(img_h,img_w)) #resizing the image\n",
        "    img=img.astype('float32') #Converting all the pixels as float32\n",
        "    train_img.append(img) #Append it to the list.\n",
        "    train_label.append(train['target'][j]) #similarly append the label\n",
        "    j=j+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 364/364 [00:02<00:00, 131.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hpU75Zamsxis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "842f8906-f6d0-43e5-83a3-a5ef1ccb1730"
      },
      "source": [
        "test_img=[]\n",
        "path='/content/drive/My Drive/dataset/test'\n",
        "for i in tqdm(test['Image']):\n",
        "    final_path=os.path.join(path,i)\n",
        "    img=cv2.imread(final_path)\n",
        "    img=cv2.resize(img,(img_h,img_w))\n",
        "    img=img.astype('float32')\n",
        "    test_img.append(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 156/156 [00:01<00:00, 124.25it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONYcuaPJtuUf",
        "colab_type": "text"
      },
      "source": [
        "### Splitting train data to train and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qc2hdtz7sxoT",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_valid,y_train,y_valid=train_test_split(train_img,train_label,test_size=0.05,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHMjpKFJtuUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "862f82a9-fa27-446f-8483-487ab77b6526"
      },
      "source": [
        "'''\n",
        "Data augmentation :- Data augmentation encompasses a wide range of techniques used to generate “new” training samples from the original ones by applying random jitters and perturbations \n",
        "\n",
        "simple geometric transforms, such as random:\n",
        "1) Translations\n",
        "2) Rotations\n",
        "4) 3) Changes in scale\n",
        "5) Shearing\n",
        "6) Horizontal (and in some cases, vertical) flips\n",
        "\n",
        "ImageDataGenerator is a method to perform data augmentation\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\nData augmentation :- Data augmentation encompasses a wide range of techniques used to generate “new” training samples from the original ones by applying random jitters and perturbations \\n\\nsimple geometric transforms, such as random:\\n1) Translations\\n2) Rotations\\n4) 3) Changes in scale\\n5) Shearing\\n6) Horizontal (and in some cases, vertical) flips\\n\\nImageDataGenerator is a method to perform data augmentation\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GsfPeJiksxct",
        "colab": {}
      },
      "source": [
        "'''Rescale is a value by which we will multiply the data before any other processing. \n",
        "Our original images consist in RGB coefficients in the 0-255, but such values would be too high for our model to process \n",
        "(given a typical learning rate), so we target values between 0 and 1 instead by scaling with a 1/255. factor'''\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,# divide each input by its std\n",
        "        rescale=1./255,\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.3, # Randomly zoom image \n",
        "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JOBlE8xhsxVt",
        "colab": {}
      },
      "source": [
        "#We don't do any changes for the test data.\n",
        "test_datagen= ImageDataGenerator(rescale=1./255)\n",
        "valid_datagen= ImageDataGenerator(rescale=1./255)\n",
        "train_datagen.fit(x_train)\n",
        "test_datagen.fit(test_img)\n",
        "valid_datagen.fit(x_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jEOpyyMtuVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e83d072c-adb4-4e89-a258-a57b6f3542f4"
      },
      "source": [
        "''' CNN works internally only with numpy arrays so we have to convert out data into numpy array  \n",
        "\n",
        "Flatten as the name implies, converts your multidimensional matrices \n",
        "(Batch.Size x Img.W x Img.H x Kernel.Size) to a nice single 2-dimensional matrix: (Batch.Size x (Img.W x Img.H x Kernel.Size)). \n",
        "\n",
        "During backpropagation it also converts back your delta of size \n",
        "(Batch.Size x (Img.W x Img.H x Kernel.Size)) to the original (Batch.Size x Img.W x Img.H x Kernel.Size).\n",
        "\n",
        "Dense layer is of course the standard fully connected layer.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "' CNN works internally only with numpy arrays so we have to convert out data into numpy array  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ui_GgGdStKiL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2c014ea4-1819-451f-b4c8-ef0f2b3f4922"
      },
      "source": [
        "train_img=np.array(train_img)\n",
        "x_train= np.array(x_train)\n",
        "x_valid= np.array(x_valid)\n",
        "y_train= np.array(y_train)\n",
        "y_valid= np.array(y_valid)\n",
        "test_img=np.array(test_img)\n",
        "train_label=np.array(train_label)\n",
        "print(\"Shape of training data=\",x_train.shape,\" and shape of labels of training data= \",y_train.shape)\n",
        "print(\"Shape of validation data=\",x_valid.shape,\" and shape of labels of validation data= \",y_valid.shape)\n",
        "print(\"Shape of test data=\",test_img.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training data= (345, 224, 224, 3)  and shape of labels of training data=  (345,)\n",
            "Shape of validation data= (19, 224, 224, 3)  and shape of labels of validation data=  (19,)\n",
            "Shape of test data= (156, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKIN_bKPtuVR",
        "colab_type": "text"
      },
      "source": [
        "## InceptionResNetV2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqXQBVdVjuMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Sequential is used to initialize the neural network.\n",
        "Convolution2D is used to make the convolutional network that deals with the images.\n",
        "MaxPooling2D layer is used to add the pooling layers.\n",
        "Flatten is the function that converts the pooled feature map to a single column that is passed to the fully connected layer.\n",
        "Dense adds the fully connected layer to the neural network.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pTjxMllStKkQ",
        "colab": {}
      },
      "source": [
        "'''Dropout regularization to avoid the overfitting\n",
        "Include_top lets you select if you want the final dense layers or not.\n",
        "the convolutional layers work as feature extractors. They identify a series of patterns in the image, \n",
        "and each layer can identify more elaborate patterns by seeing patterns of patterns.\n",
        "the dense layers are capable of interpreting the found patterns in order to classify: this image contains cats, dogs, cars, etc.\n",
        "\n",
        "So the dence layers or last ouputting layers need to be manullay add if we are doing the feature engineering.\n",
        "ImageNet is a benchmark dataset containing millions of images and its weights are very good for many of computer vision tasks.\n",
        "Both input_tenser and input_shape can't be set arguments any one of its must an argument.Both specifies user specicified input size\n",
        "'''\n",
        "\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from tensorflow.keras.layers import Dropout\n",
        "base_model= InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=None, input_shape=(img_h,img_w,3), pooling='avg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AUbQtrTatKpg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37ad99c0-e1d0-43db-9177-c8f0a5128182"
      },
      "source": [
        "#make sure this layers won't gets trained.As they almost have a good weights.\n",
        "base_model.trainable=False\n",
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_resnet_v2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 111, 111, 32) 864         input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 111, 111, 32) 96          conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 111, 111, 32) 0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 109, 109, 32) 9216        activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 109, 109, 32) 96          conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 109, 109, 32) 0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 109, 109, 64) 18432       activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 109, 109, 64) 192         conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 109, 109, 64) 0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 54, 54, 64)   0           activation_205[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 54, 54, 80)   5120        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 54, 54, 80)   240         conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 54, 54, 80)   0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 52, 52, 192)  138240      activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 52, 52, 192)  576         conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 52, 52, 192)  0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_207[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 25, 25, 64)   192         conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 25, 25, 64)   0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 25, 25, 48)   9216        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 25, 25, 96)   55296       activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 25, 25, 48)   144         conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 25, 25, 96)   288         conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 25, 25, 48)   0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 25, 25, 96)   0           batch_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 25, 25, 192)  0           max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 25, 25, 96)   18432       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 25, 25, 64)   76800       activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 25, 25, 96)   82944       activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 25, 25, 64)   12288       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 25, 25, 96)   288         conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 25, 25, 64)   192         conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 25, 25, 96)   288         conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 25, 25, 64)   192         conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 25, 25, 96)   0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 25, 25, 64)   0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 25, 25, 96)   0           batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 25, 25, 64)   0           batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed_5b (Concatenate)          (None, 25, 25, 320)  0           activation_208[0][0]             \n",
            "                                                                 activation_210[0][0]             \n",
            "                                                                 activation_213[0][0]             \n",
            "                                                                 activation_214[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 25, 25, 32)   10240       mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 25, 25, 32)   96          conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 25, 25, 32)   0           batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 25, 25, 32)   10240       mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 25, 25, 48)   13824       activation_218[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 25, 25, 32)   96          conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 25, 25, 48)   144         conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 25, 25, 32)   0           batch_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 25, 25, 48)   0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 25, 25, 32)   10240       mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 25, 25, 32)   9216        activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 25, 25, 64)   27648       activation_219[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 25, 25, 32)   96          conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 25, 25, 32)   96          conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 25, 25, 64)   192         conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 25, 25, 32)   0           batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 25, 25, 32)   0           batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 25, 25, 64)   0           batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block35_1_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_215[0][0]             \n",
            "                                                                 activation_217[0][0]             \n",
            "                                                                 activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_1_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_1_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_1 (Lambda)              (None, 25, 25, 320)  0           mixed_5b[0][0]                   \n",
            "                                                                 block35_1_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_1_ac (Activation)       (None, 25, 25, 320)  0           block35_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 25, 25, 32)   10240       block35_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 25, 25, 32)   96          conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 25, 25, 32)   0           batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 25, 25, 32)   10240       block35_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 25, 25, 48)   13824       activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 25, 25, 32)   96          conv2d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 25, 25, 48)   144         conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 25, 25, 32)   0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 25, 25, 48)   0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 25, 25, 32)   10240       block35_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 25, 25, 32)   9216        activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 25, 25, 64)   27648       activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 25, 25, 32)   96          conv2d_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 25, 25, 32)   96          conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 25, 25, 64)   192         conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 25, 25, 32)   0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 25, 25, 32)   0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 25, 25, 64)   0           batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block35_2_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_221[0][0]             \n",
            "                                                                 activation_223[0][0]             \n",
            "                                                                 activation_226[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_2_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_2_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_2 (Lambda)              (None, 25, 25, 320)  0           block35_1_ac[0][0]               \n",
            "                                                                 block35_2_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_2_ac (Activation)       (None, 25, 25, 320)  0           block35_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 25, 25, 32)   10240       block35_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 25, 25, 32)   96          conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 25, 25, 32)   0           batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 25, 25, 32)   10240       block35_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 25, 25, 48)   13824       activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 25, 25, 32)   96          conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 25, 25, 48)   144         conv2d_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 25, 25, 32)   0           batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 25, 25, 48)   0           batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 25, 25, 32)   10240       block35_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 25, 25, 32)   9216        activation_228[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 25, 25, 64)   27648       activation_231[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 25, 25, 32)   96          conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 25, 25, 32)   96          conv2d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 25, 25, 64)   192         conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 25, 25, 32)   0           batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 25, 25, 32)   0           batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 25, 25, 64)   0           batch_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block35_3_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_227[0][0]             \n",
            "                                                                 activation_229[0][0]             \n",
            "                                                                 activation_232[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_3_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_3_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_3 (Lambda)              (None, 25, 25, 320)  0           block35_2_ac[0][0]               \n",
            "                                                                 block35_3_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_3_ac (Activation)       (None, 25, 25, 320)  0           block35_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 25, 25, 32)   10240       block35_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_242 (BatchN (None, 25, 25, 32)   96          conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_236 (Activation)     (None, 25, 25, 32)   0           batch_normalization_242[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 25, 25, 32)   10240       block35_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 25, 25, 48)   13824       activation_236[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_240 (BatchN (None, 25, 25, 32)   96          conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_243 (BatchN (None, 25, 25, 48)   144         conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_234 (Activation)     (None, 25, 25, 32)   0           batch_normalization_240[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_237 (Activation)     (None, 25, 25, 48)   0           batch_normalization_243[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 25, 25, 32)   10240       block35_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 25, 25, 32)   9216        activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 25, 25, 64)   27648       activation_237[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_239 (BatchN (None, 25, 25, 32)   96          conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_241 (BatchN (None, 25, 25, 32)   96          conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_244 (BatchN (None, 25, 25, 64)   192         conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 25, 25, 32)   0           batch_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_235 (Activation)     (None, 25, 25, 32)   0           batch_normalization_241[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_238 (Activation)     (None, 25, 25, 64)   0           batch_normalization_244[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block35_4_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_233[0][0]             \n",
            "                                                                 activation_235[0][0]             \n",
            "                                                                 activation_238[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_4_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_4_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_4 (Lambda)              (None, 25, 25, 320)  0           block35_3_ac[0][0]               \n",
            "                                                                 block35_4_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_4_ac (Activation)       (None, 25, 25, 320)  0           block35_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 25, 25, 32)   10240       block35_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_248 (BatchN (None, 25, 25, 32)   96          conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_242 (Activation)     (None, 25, 25, 32)   0           batch_normalization_248[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 25, 25, 32)   10240       block35_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_243 (Conv2D)             (None, 25, 25, 48)   13824       activation_242[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_246 (BatchN (None, 25, 25, 32)   96          conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_249 (BatchN (None, 25, 25, 48)   144         conv2d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_240 (Activation)     (None, 25, 25, 32)   0           batch_normalization_246[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_243 (Activation)     (None, 25, 25, 48)   0           batch_normalization_249[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 25, 25, 32)   10240       block35_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 25, 25, 32)   9216        activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_244 (Conv2D)             (None, 25, 25, 64)   27648       activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_245 (BatchN (None, 25, 25, 32)   96          conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_247 (BatchN (None, 25, 25, 32)   96          conv2d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_250 (BatchN (None, 25, 25, 64)   192         conv2d_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_239 (Activation)     (None, 25, 25, 32)   0           batch_normalization_245[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_241 (Activation)     (None, 25, 25, 32)   0           batch_normalization_247[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_244 (Activation)     (None, 25, 25, 64)   0           batch_normalization_250[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block35_5_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_239[0][0]             \n",
            "                                                                 activation_241[0][0]             \n",
            "                                                                 activation_244[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_5_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_5_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_5 (Lambda)              (None, 25, 25, 320)  0           block35_4_ac[0][0]               \n",
            "                                                                 block35_5_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_5_ac (Activation)       (None, 25, 25, 320)  0           block35_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 25, 25, 32)   10240       block35_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_254 (BatchN (None, 25, 25, 32)   96          conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_248 (Activation)     (None, 25, 25, 32)   0           batch_normalization_254[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 25, 25, 32)   10240       block35_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 25, 25, 48)   13824       activation_248[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_252 (BatchN (None, 25, 25, 32)   96          conv2d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_255 (BatchN (None, 25, 25, 48)   144         conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_246 (Activation)     (None, 25, 25, 32)   0           batch_normalization_252[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_249 (Activation)     (None, 25, 25, 48)   0           batch_normalization_255[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 25, 25, 32)   10240       block35_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 25, 25, 32)   9216        activation_246[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 25, 25, 64)   27648       activation_249[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_251 (BatchN (None, 25, 25, 32)   96          conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_253 (BatchN (None, 25, 25, 32)   96          conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_256 (BatchN (None, 25, 25, 64)   192         conv2d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_245 (Activation)     (None, 25, 25, 32)   0           batch_normalization_251[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_247 (Activation)     (None, 25, 25, 32)   0           batch_normalization_253[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_250 (Activation)     (None, 25, 25, 64)   0           batch_normalization_256[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block35_6_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_245[0][0]             \n",
            "                                                                 activation_247[0][0]             \n",
            "                                                                 activation_250[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_6_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_6_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_6 (Lambda)              (None, 25, 25, 320)  0           block35_5_ac[0][0]               \n",
            "                                                                 block35_6_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_6_ac (Activation)       (None, 25, 25, 320)  0           block35_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 25, 25, 32)   10240       block35_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_260 (BatchN (None, 25, 25, 32)   96          conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_254 (Activation)     (None, 25, 25, 32)   0           batch_normalization_260[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 25, 25, 32)   10240       block35_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_255 (Conv2D)             (None, 25, 25, 48)   13824       activation_254[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_258 (BatchN (None, 25, 25, 32)   96          conv2d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_261 (BatchN (None, 25, 25, 48)   144         conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_252 (Activation)     (None, 25, 25, 32)   0           batch_normalization_258[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_255 (Activation)     (None, 25, 25, 48)   0           batch_normalization_261[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 25, 25, 32)   10240       block35_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 25, 25, 32)   9216        activation_252[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_256 (Conv2D)             (None, 25, 25, 64)   27648       activation_255[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_257 (BatchN (None, 25, 25, 32)   96          conv2d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_259 (BatchN (None, 25, 25, 32)   96          conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_262 (BatchN (None, 25, 25, 64)   192         conv2d_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_251 (Activation)     (None, 25, 25, 32)   0           batch_normalization_257[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_253 (Activation)     (None, 25, 25, 32)   0           batch_normalization_259[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_256 (Activation)     (None, 25, 25, 64)   0           batch_normalization_262[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block35_7_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_251[0][0]             \n",
            "                                                                 activation_253[0][0]             \n",
            "                                                                 activation_256[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_7_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_7_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_7 (Lambda)              (None, 25, 25, 320)  0           block35_6_ac[0][0]               \n",
            "                                                                 block35_7_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_7_ac (Activation)       (None, 25, 25, 320)  0           block35_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_260 (Conv2D)             (None, 25, 25, 32)   10240       block35_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_266 (BatchN (None, 25, 25, 32)   96          conv2d_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_260 (Activation)     (None, 25, 25, 32)   0           batch_normalization_266[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_258 (Conv2D)             (None, 25, 25, 32)   10240       block35_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_261 (Conv2D)             (None, 25, 25, 48)   13824       activation_260[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_264 (BatchN (None, 25, 25, 32)   96          conv2d_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_267 (BatchN (None, 25, 25, 48)   144         conv2d_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_258 (Activation)     (None, 25, 25, 32)   0           batch_normalization_264[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_261 (Activation)     (None, 25, 25, 48)   0           batch_normalization_267[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_257 (Conv2D)             (None, 25, 25, 32)   10240       block35_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_259 (Conv2D)             (None, 25, 25, 32)   9216        activation_258[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_262 (Conv2D)             (None, 25, 25, 64)   27648       activation_261[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_263 (BatchN (None, 25, 25, 32)   96          conv2d_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_265 (BatchN (None, 25, 25, 32)   96          conv2d_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_268 (BatchN (None, 25, 25, 64)   192         conv2d_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_257 (Activation)     (None, 25, 25, 32)   0           batch_normalization_263[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_259 (Activation)     (None, 25, 25, 32)   0           batch_normalization_265[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_262 (Activation)     (None, 25, 25, 64)   0           batch_normalization_268[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block35_8_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_257[0][0]             \n",
            "                                                                 activation_259[0][0]             \n",
            "                                                                 activation_262[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_8_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_8_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_8 (Lambda)              (None, 25, 25, 320)  0           block35_7_ac[0][0]               \n",
            "                                                                 block35_8_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_8_ac (Activation)       (None, 25, 25, 320)  0           block35_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_266 (Conv2D)             (None, 25, 25, 32)   10240       block35_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_272 (BatchN (None, 25, 25, 32)   96          conv2d_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_266 (Activation)     (None, 25, 25, 32)   0           batch_normalization_272[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_264 (Conv2D)             (None, 25, 25, 32)   10240       block35_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_267 (Conv2D)             (None, 25, 25, 48)   13824       activation_266[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_270 (BatchN (None, 25, 25, 32)   96          conv2d_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_273 (BatchN (None, 25, 25, 48)   144         conv2d_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_264 (Activation)     (None, 25, 25, 32)   0           batch_normalization_270[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_267 (Activation)     (None, 25, 25, 48)   0           batch_normalization_273[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_263 (Conv2D)             (None, 25, 25, 32)   10240       block35_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_265 (Conv2D)             (None, 25, 25, 32)   9216        activation_264[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_268 (Conv2D)             (None, 25, 25, 64)   27648       activation_267[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_269 (BatchN (None, 25, 25, 32)   96          conv2d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_271 (BatchN (None, 25, 25, 32)   96          conv2d_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_274 (BatchN (None, 25, 25, 64)   192         conv2d_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_263 (Activation)     (None, 25, 25, 32)   0           batch_normalization_269[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_265 (Activation)     (None, 25, 25, 32)   0           batch_normalization_271[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_268 (Activation)     (None, 25, 25, 64)   0           batch_normalization_274[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block35_9_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_263[0][0]             \n",
            "                                                                 activation_265[0][0]             \n",
            "                                                                 activation_268[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_9_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_9_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_9 (Lambda)              (None, 25, 25, 320)  0           block35_8_ac[0][0]               \n",
            "                                                                 block35_9_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_9_ac (Activation)       (None, 25, 25, 320)  0           block35_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_272 (Conv2D)             (None, 25, 25, 32)   10240       block35_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_278 (BatchN (None, 25, 25, 32)   96          conv2d_272[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_272 (Activation)     (None, 25, 25, 32)   0           batch_normalization_278[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_270 (Conv2D)             (None, 25, 25, 32)   10240       block35_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_273 (Conv2D)             (None, 25, 25, 48)   13824       activation_272[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_276 (BatchN (None, 25, 25, 32)   96          conv2d_270[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_279 (BatchN (None, 25, 25, 48)   144         conv2d_273[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_270 (Activation)     (None, 25, 25, 32)   0           batch_normalization_276[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_273 (Activation)     (None, 25, 25, 48)   0           batch_normalization_279[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_269 (Conv2D)             (None, 25, 25, 32)   10240       block35_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_271 (Conv2D)             (None, 25, 25, 32)   9216        activation_270[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_274 (Conv2D)             (None, 25, 25, 64)   27648       activation_273[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_275 (BatchN (None, 25, 25, 32)   96          conv2d_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_277 (BatchN (None, 25, 25, 32)   96          conv2d_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_280 (BatchN (None, 25, 25, 64)   192         conv2d_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_269 (Activation)     (None, 25, 25, 32)   0           batch_normalization_275[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_271 (Activation)     (None, 25, 25, 32)   0           batch_normalization_277[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_274 (Activation)     (None, 25, 25, 64)   0           batch_normalization_280[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block35_10_mixed (Concatenate)  (None, 25, 25, 128)  0           activation_269[0][0]             \n",
            "                                                                 activation_271[0][0]             \n",
            "                                                                 activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_10_conv (Conv2D)        (None, 25, 25, 320)  41280       block35_10_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block35_10 (Lambda)             (None, 25, 25, 320)  0           block35_9_ac[0][0]               \n",
            "                                                                 block35_10_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_10_ac (Activation)      (None, 25, 25, 320)  0           block35_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_276 (Conv2D)             (None, 25, 25, 256)  81920       block35_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_282 (BatchN (None, 25, 25, 256)  768         conv2d_276[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_276 (Activation)     (None, 25, 25, 256)  0           batch_normalization_282[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_277 (Conv2D)             (None, 25, 25, 256)  589824      activation_276[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_283 (BatchN (None, 25, 25, 256)  768         conv2d_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_277 (Activation)     (None, 25, 25, 256)  0           batch_normalization_283[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_275 (Conv2D)             (None, 12, 12, 384)  1105920     block35_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_278 (Conv2D)             (None, 12, 12, 384)  884736      activation_277[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_281 (BatchN (None, 12, 12, 384)  1152        conv2d_275[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_284 (BatchN (None, 12, 12, 384)  1152        conv2d_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_275 (Activation)     (None, 12, 12, 384)  0           batch_normalization_281[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_278 (Activation)     (None, 12, 12, 384)  0           batch_normalization_284[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 12, 12, 320)  0           block35_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "mixed_6a (Concatenate)          (None, 12, 12, 1088) 0           activation_275[0][0]             \n",
            "                                                                 activation_278[0][0]             \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_280 (Conv2D)             (None, 12, 12, 128)  139264      mixed_6a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_286 (BatchN (None, 12, 12, 128)  384         conv2d_280[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_280 (Activation)     (None, 12, 12, 128)  0           batch_normalization_286[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_281 (Conv2D)             (None, 12, 12, 160)  143360      activation_280[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_287 (BatchN (None, 12, 12, 160)  480         conv2d_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_281 (Activation)     (None, 12, 12, 160)  0           batch_normalization_287[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_279 (Conv2D)             (None, 12, 12, 192)  208896      mixed_6a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_282 (Conv2D)             (None, 12, 12, 192)  215040      activation_281[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_285 (BatchN (None, 12, 12, 192)  576         conv2d_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_288 (BatchN (None, 12, 12, 192)  576         conv2d_282[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_279 (Activation)     (None, 12, 12, 192)  0           batch_normalization_285[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_282 (Activation)     (None, 12, 12, 192)  0           batch_normalization_288[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_1_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_279[0][0]             \n",
            "                                                                 activation_282[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_1_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_1_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_1 (Lambda)              (None, 12, 12, 1088) 0           mixed_6a[0][0]                   \n",
            "                                                                 block17_1_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_1_ac (Activation)       (None, 12, 12, 1088) 0           block17_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_284 (Conv2D)             (None, 12, 12, 128)  139264      block17_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_290 (BatchN (None, 12, 12, 128)  384         conv2d_284[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_284 (Activation)     (None, 12, 12, 128)  0           batch_normalization_290[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_285 (Conv2D)             (None, 12, 12, 160)  143360      activation_284[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_291 (BatchN (None, 12, 12, 160)  480         conv2d_285[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_285 (Activation)     (None, 12, 12, 160)  0           batch_normalization_291[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_283 (Conv2D)             (None, 12, 12, 192)  208896      block17_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_286 (Conv2D)             (None, 12, 12, 192)  215040      activation_285[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_289 (BatchN (None, 12, 12, 192)  576         conv2d_283[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_292 (BatchN (None, 12, 12, 192)  576         conv2d_286[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_283 (Activation)     (None, 12, 12, 192)  0           batch_normalization_289[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_286 (Activation)     (None, 12, 12, 192)  0           batch_normalization_292[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_2_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_283[0][0]             \n",
            "                                                                 activation_286[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_2_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_2_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_2 (Lambda)              (None, 12, 12, 1088) 0           block17_1_ac[0][0]               \n",
            "                                                                 block17_2_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_2_ac (Activation)       (None, 12, 12, 1088) 0           block17_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_288 (Conv2D)             (None, 12, 12, 128)  139264      block17_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_294 (BatchN (None, 12, 12, 128)  384         conv2d_288[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_288 (Activation)     (None, 12, 12, 128)  0           batch_normalization_294[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_289 (Conv2D)             (None, 12, 12, 160)  143360      activation_288[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_295 (BatchN (None, 12, 12, 160)  480         conv2d_289[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_289 (Activation)     (None, 12, 12, 160)  0           batch_normalization_295[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_287 (Conv2D)             (None, 12, 12, 192)  208896      block17_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_290 (Conv2D)             (None, 12, 12, 192)  215040      activation_289[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_293 (BatchN (None, 12, 12, 192)  576         conv2d_287[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_296 (BatchN (None, 12, 12, 192)  576         conv2d_290[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_287 (Activation)     (None, 12, 12, 192)  0           batch_normalization_293[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_290 (Activation)     (None, 12, 12, 192)  0           batch_normalization_296[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_3_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_287[0][0]             \n",
            "                                                                 activation_290[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_3_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_3_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_3 (Lambda)              (None, 12, 12, 1088) 0           block17_2_ac[0][0]               \n",
            "                                                                 block17_3_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_3_ac (Activation)       (None, 12, 12, 1088) 0           block17_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_292 (Conv2D)             (None, 12, 12, 128)  139264      block17_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_298 (BatchN (None, 12, 12, 128)  384         conv2d_292[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_292 (Activation)     (None, 12, 12, 128)  0           batch_normalization_298[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_293 (Conv2D)             (None, 12, 12, 160)  143360      activation_292[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_299 (BatchN (None, 12, 12, 160)  480         conv2d_293[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_293 (Activation)     (None, 12, 12, 160)  0           batch_normalization_299[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_291 (Conv2D)             (None, 12, 12, 192)  208896      block17_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_294 (Conv2D)             (None, 12, 12, 192)  215040      activation_293[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_297 (BatchN (None, 12, 12, 192)  576         conv2d_291[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_300 (BatchN (None, 12, 12, 192)  576         conv2d_294[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_291 (Activation)     (None, 12, 12, 192)  0           batch_normalization_297[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_294 (Activation)     (None, 12, 12, 192)  0           batch_normalization_300[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_4_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_291[0][0]             \n",
            "                                                                 activation_294[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_4_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_4_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_4 (Lambda)              (None, 12, 12, 1088) 0           block17_3_ac[0][0]               \n",
            "                                                                 block17_4_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_4_ac (Activation)       (None, 12, 12, 1088) 0           block17_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_296 (Conv2D)             (None, 12, 12, 128)  139264      block17_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_302 (BatchN (None, 12, 12, 128)  384         conv2d_296[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_296 (Activation)     (None, 12, 12, 128)  0           batch_normalization_302[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_297 (Conv2D)             (None, 12, 12, 160)  143360      activation_296[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_303 (BatchN (None, 12, 12, 160)  480         conv2d_297[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_297 (Activation)     (None, 12, 12, 160)  0           batch_normalization_303[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_295 (Conv2D)             (None, 12, 12, 192)  208896      block17_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_298 (Conv2D)             (None, 12, 12, 192)  215040      activation_297[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_301 (BatchN (None, 12, 12, 192)  576         conv2d_295[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_304 (BatchN (None, 12, 12, 192)  576         conv2d_298[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_295 (Activation)     (None, 12, 12, 192)  0           batch_normalization_301[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_298 (Activation)     (None, 12, 12, 192)  0           batch_normalization_304[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_5_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_295[0][0]             \n",
            "                                                                 activation_298[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_5_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_5_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_5 (Lambda)              (None, 12, 12, 1088) 0           block17_4_ac[0][0]               \n",
            "                                                                 block17_5_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_5_ac (Activation)       (None, 12, 12, 1088) 0           block17_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_300 (Conv2D)             (None, 12, 12, 128)  139264      block17_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_306 (BatchN (None, 12, 12, 128)  384         conv2d_300[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_300 (Activation)     (None, 12, 12, 128)  0           batch_normalization_306[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_301 (Conv2D)             (None, 12, 12, 160)  143360      activation_300[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_307 (BatchN (None, 12, 12, 160)  480         conv2d_301[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_301 (Activation)     (None, 12, 12, 160)  0           batch_normalization_307[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_299 (Conv2D)             (None, 12, 12, 192)  208896      block17_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_302 (Conv2D)             (None, 12, 12, 192)  215040      activation_301[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_305 (BatchN (None, 12, 12, 192)  576         conv2d_299[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_308 (BatchN (None, 12, 12, 192)  576         conv2d_302[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_299 (Activation)     (None, 12, 12, 192)  0           batch_normalization_305[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_302 (Activation)     (None, 12, 12, 192)  0           batch_normalization_308[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_6_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_299[0][0]             \n",
            "                                                                 activation_302[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_6_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_6_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_6 (Lambda)              (None, 12, 12, 1088) 0           block17_5_ac[0][0]               \n",
            "                                                                 block17_6_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_6_ac (Activation)       (None, 12, 12, 1088) 0           block17_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_304 (Conv2D)             (None, 12, 12, 128)  139264      block17_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_310 (BatchN (None, 12, 12, 128)  384         conv2d_304[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_304 (Activation)     (None, 12, 12, 128)  0           batch_normalization_310[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_305 (Conv2D)             (None, 12, 12, 160)  143360      activation_304[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_311 (BatchN (None, 12, 12, 160)  480         conv2d_305[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_305 (Activation)     (None, 12, 12, 160)  0           batch_normalization_311[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_303 (Conv2D)             (None, 12, 12, 192)  208896      block17_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_306 (Conv2D)             (None, 12, 12, 192)  215040      activation_305[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_309 (BatchN (None, 12, 12, 192)  576         conv2d_303[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_312 (BatchN (None, 12, 12, 192)  576         conv2d_306[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_303 (Activation)     (None, 12, 12, 192)  0           batch_normalization_309[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_306 (Activation)     (None, 12, 12, 192)  0           batch_normalization_312[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_7_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_303[0][0]             \n",
            "                                                                 activation_306[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_7_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_7_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_7 (Lambda)              (None, 12, 12, 1088) 0           block17_6_ac[0][0]               \n",
            "                                                                 block17_7_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_7_ac (Activation)       (None, 12, 12, 1088) 0           block17_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_308 (Conv2D)             (None, 12, 12, 128)  139264      block17_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_314 (BatchN (None, 12, 12, 128)  384         conv2d_308[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_308 (Activation)     (None, 12, 12, 128)  0           batch_normalization_314[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_309 (Conv2D)             (None, 12, 12, 160)  143360      activation_308[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_315 (BatchN (None, 12, 12, 160)  480         conv2d_309[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_309 (Activation)     (None, 12, 12, 160)  0           batch_normalization_315[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_307 (Conv2D)             (None, 12, 12, 192)  208896      block17_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_310 (Conv2D)             (None, 12, 12, 192)  215040      activation_309[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_313 (BatchN (None, 12, 12, 192)  576         conv2d_307[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_316 (BatchN (None, 12, 12, 192)  576         conv2d_310[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_307 (Activation)     (None, 12, 12, 192)  0           batch_normalization_313[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_310 (Activation)     (None, 12, 12, 192)  0           batch_normalization_316[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_8_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_307[0][0]             \n",
            "                                                                 activation_310[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_8_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_8_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_8 (Lambda)              (None, 12, 12, 1088) 0           block17_7_ac[0][0]               \n",
            "                                                                 block17_8_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_8_ac (Activation)       (None, 12, 12, 1088) 0           block17_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_312 (Conv2D)             (None, 12, 12, 128)  139264      block17_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_318 (BatchN (None, 12, 12, 128)  384         conv2d_312[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_312 (Activation)     (None, 12, 12, 128)  0           batch_normalization_318[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_313 (Conv2D)             (None, 12, 12, 160)  143360      activation_312[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_319 (BatchN (None, 12, 12, 160)  480         conv2d_313[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_313 (Activation)     (None, 12, 12, 160)  0           batch_normalization_319[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_311 (Conv2D)             (None, 12, 12, 192)  208896      block17_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_314 (Conv2D)             (None, 12, 12, 192)  215040      activation_313[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_317 (BatchN (None, 12, 12, 192)  576         conv2d_311[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_320 (BatchN (None, 12, 12, 192)  576         conv2d_314[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_311 (Activation)     (None, 12, 12, 192)  0           batch_normalization_317[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_314 (Activation)     (None, 12, 12, 192)  0           batch_normalization_320[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_9_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_311[0][0]             \n",
            "                                                                 activation_314[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_9_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_9_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_9 (Lambda)              (None, 12, 12, 1088) 0           block17_8_ac[0][0]               \n",
            "                                                                 block17_9_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_9_ac (Activation)       (None, 12, 12, 1088) 0           block17_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_316 (Conv2D)             (None, 12, 12, 128)  139264      block17_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_322 (BatchN (None, 12, 12, 128)  384         conv2d_316[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_316 (Activation)     (None, 12, 12, 128)  0           batch_normalization_322[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_317 (Conv2D)             (None, 12, 12, 160)  143360      activation_316[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_323 (BatchN (None, 12, 12, 160)  480         conv2d_317[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_317 (Activation)     (None, 12, 12, 160)  0           batch_normalization_323[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_315 (Conv2D)             (None, 12, 12, 192)  208896      block17_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_318 (Conv2D)             (None, 12, 12, 192)  215040      activation_317[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_321 (BatchN (None, 12, 12, 192)  576         conv2d_315[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_324 (BatchN (None, 12, 12, 192)  576         conv2d_318[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_315 (Activation)     (None, 12, 12, 192)  0           batch_normalization_321[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_318 (Activation)     (None, 12, 12, 192)  0           batch_normalization_324[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_10_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_315[0][0]             \n",
            "                                                                 activation_318[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_10_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_10_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_10 (Lambda)             (None, 12, 12, 1088) 0           block17_9_ac[0][0]               \n",
            "                                                                 block17_10_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_10_ac (Activation)      (None, 12, 12, 1088) 0           block17_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_320 (Conv2D)             (None, 12, 12, 128)  139264      block17_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_326 (BatchN (None, 12, 12, 128)  384         conv2d_320[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_320 (Activation)     (None, 12, 12, 128)  0           batch_normalization_326[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_321 (Conv2D)             (None, 12, 12, 160)  143360      activation_320[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_327 (BatchN (None, 12, 12, 160)  480         conv2d_321[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_321 (Activation)     (None, 12, 12, 160)  0           batch_normalization_327[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_319 (Conv2D)             (None, 12, 12, 192)  208896      block17_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_322 (Conv2D)             (None, 12, 12, 192)  215040      activation_321[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_325 (BatchN (None, 12, 12, 192)  576         conv2d_319[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_328 (BatchN (None, 12, 12, 192)  576         conv2d_322[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_319 (Activation)     (None, 12, 12, 192)  0           batch_normalization_325[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_322 (Activation)     (None, 12, 12, 192)  0           batch_normalization_328[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_11_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_319[0][0]             \n",
            "                                                                 activation_322[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_11_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_11_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_11 (Lambda)             (None, 12, 12, 1088) 0           block17_10_ac[0][0]              \n",
            "                                                                 block17_11_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_11_ac (Activation)      (None, 12, 12, 1088) 0           block17_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_324 (Conv2D)             (None, 12, 12, 128)  139264      block17_11_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_330 (BatchN (None, 12, 12, 128)  384         conv2d_324[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_324 (Activation)     (None, 12, 12, 128)  0           batch_normalization_330[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_325 (Conv2D)             (None, 12, 12, 160)  143360      activation_324[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_331 (BatchN (None, 12, 12, 160)  480         conv2d_325[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_325 (Activation)     (None, 12, 12, 160)  0           batch_normalization_331[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_323 (Conv2D)             (None, 12, 12, 192)  208896      block17_11_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_326 (Conv2D)             (None, 12, 12, 192)  215040      activation_325[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_329 (BatchN (None, 12, 12, 192)  576         conv2d_323[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_332 (BatchN (None, 12, 12, 192)  576         conv2d_326[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_323 (Activation)     (None, 12, 12, 192)  0           batch_normalization_329[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_326 (Activation)     (None, 12, 12, 192)  0           batch_normalization_332[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_12_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_323[0][0]             \n",
            "                                                                 activation_326[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_12_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_12_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_12 (Lambda)             (None, 12, 12, 1088) 0           block17_11_ac[0][0]              \n",
            "                                                                 block17_12_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_12_ac (Activation)      (None, 12, 12, 1088) 0           block17_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_328 (Conv2D)             (None, 12, 12, 128)  139264      block17_12_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_334 (BatchN (None, 12, 12, 128)  384         conv2d_328[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_328 (Activation)     (None, 12, 12, 128)  0           batch_normalization_334[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_329 (Conv2D)             (None, 12, 12, 160)  143360      activation_328[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_335 (BatchN (None, 12, 12, 160)  480         conv2d_329[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_329 (Activation)     (None, 12, 12, 160)  0           batch_normalization_335[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_327 (Conv2D)             (None, 12, 12, 192)  208896      block17_12_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_330 (Conv2D)             (None, 12, 12, 192)  215040      activation_329[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_333 (BatchN (None, 12, 12, 192)  576         conv2d_327[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_336 (BatchN (None, 12, 12, 192)  576         conv2d_330[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_327 (Activation)     (None, 12, 12, 192)  0           batch_normalization_333[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_330 (Activation)     (None, 12, 12, 192)  0           batch_normalization_336[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_13_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_327[0][0]             \n",
            "                                                                 activation_330[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_13_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_13_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_13 (Lambda)             (None, 12, 12, 1088) 0           block17_12_ac[0][0]              \n",
            "                                                                 block17_13_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_13_ac (Activation)      (None, 12, 12, 1088) 0           block17_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_332 (Conv2D)             (None, 12, 12, 128)  139264      block17_13_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_338 (BatchN (None, 12, 12, 128)  384         conv2d_332[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_332 (Activation)     (None, 12, 12, 128)  0           batch_normalization_338[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_333 (Conv2D)             (None, 12, 12, 160)  143360      activation_332[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_339 (BatchN (None, 12, 12, 160)  480         conv2d_333[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_333 (Activation)     (None, 12, 12, 160)  0           batch_normalization_339[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_331 (Conv2D)             (None, 12, 12, 192)  208896      block17_13_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_334 (Conv2D)             (None, 12, 12, 192)  215040      activation_333[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_337 (BatchN (None, 12, 12, 192)  576         conv2d_331[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_340 (BatchN (None, 12, 12, 192)  576         conv2d_334[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_331 (Activation)     (None, 12, 12, 192)  0           batch_normalization_337[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_334 (Activation)     (None, 12, 12, 192)  0           batch_normalization_340[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_14_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_331[0][0]             \n",
            "                                                                 activation_334[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_14_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_14_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_14 (Lambda)             (None, 12, 12, 1088) 0           block17_13_ac[0][0]              \n",
            "                                                                 block17_14_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_14_ac (Activation)      (None, 12, 12, 1088) 0           block17_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_336 (Conv2D)             (None, 12, 12, 128)  139264      block17_14_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_342 (BatchN (None, 12, 12, 128)  384         conv2d_336[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_336 (Activation)     (None, 12, 12, 128)  0           batch_normalization_342[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_337 (Conv2D)             (None, 12, 12, 160)  143360      activation_336[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_343 (BatchN (None, 12, 12, 160)  480         conv2d_337[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_337 (Activation)     (None, 12, 12, 160)  0           batch_normalization_343[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_335 (Conv2D)             (None, 12, 12, 192)  208896      block17_14_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_338 (Conv2D)             (None, 12, 12, 192)  215040      activation_337[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_341 (BatchN (None, 12, 12, 192)  576         conv2d_335[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_344 (BatchN (None, 12, 12, 192)  576         conv2d_338[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_335 (Activation)     (None, 12, 12, 192)  0           batch_normalization_341[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_338 (Activation)     (None, 12, 12, 192)  0           batch_normalization_344[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_15_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_335[0][0]             \n",
            "                                                                 activation_338[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_15_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_15_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_15 (Lambda)             (None, 12, 12, 1088) 0           block17_14_ac[0][0]              \n",
            "                                                                 block17_15_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_15_ac (Activation)      (None, 12, 12, 1088) 0           block17_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_340 (Conv2D)             (None, 12, 12, 128)  139264      block17_15_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_346 (BatchN (None, 12, 12, 128)  384         conv2d_340[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_340 (Activation)     (None, 12, 12, 128)  0           batch_normalization_346[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_341 (Conv2D)             (None, 12, 12, 160)  143360      activation_340[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_347 (BatchN (None, 12, 12, 160)  480         conv2d_341[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_341 (Activation)     (None, 12, 12, 160)  0           batch_normalization_347[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_339 (Conv2D)             (None, 12, 12, 192)  208896      block17_15_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_342 (Conv2D)             (None, 12, 12, 192)  215040      activation_341[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_345 (BatchN (None, 12, 12, 192)  576         conv2d_339[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_348 (BatchN (None, 12, 12, 192)  576         conv2d_342[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_339 (Activation)     (None, 12, 12, 192)  0           batch_normalization_345[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_342 (Activation)     (None, 12, 12, 192)  0           batch_normalization_348[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_16_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_339[0][0]             \n",
            "                                                                 activation_342[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_16_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_16_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_16 (Lambda)             (None, 12, 12, 1088) 0           block17_15_ac[0][0]              \n",
            "                                                                 block17_16_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_16_ac (Activation)      (None, 12, 12, 1088) 0           block17_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_344 (Conv2D)             (None, 12, 12, 128)  139264      block17_16_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_350 (BatchN (None, 12, 12, 128)  384         conv2d_344[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_344 (Activation)     (None, 12, 12, 128)  0           batch_normalization_350[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_345 (Conv2D)             (None, 12, 12, 160)  143360      activation_344[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_351 (BatchN (None, 12, 12, 160)  480         conv2d_345[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_345 (Activation)     (None, 12, 12, 160)  0           batch_normalization_351[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_343 (Conv2D)             (None, 12, 12, 192)  208896      block17_16_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_346 (Conv2D)             (None, 12, 12, 192)  215040      activation_345[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_349 (BatchN (None, 12, 12, 192)  576         conv2d_343[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_352 (BatchN (None, 12, 12, 192)  576         conv2d_346[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_343 (Activation)     (None, 12, 12, 192)  0           batch_normalization_349[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_346 (Activation)     (None, 12, 12, 192)  0           batch_normalization_352[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_17_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_343[0][0]             \n",
            "                                                                 activation_346[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_17_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_17_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_17 (Lambda)             (None, 12, 12, 1088) 0           block17_16_ac[0][0]              \n",
            "                                                                 block17_17_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_17_ac (Activation)      (None, 12, 12, 1088) 0           block17_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_348 (Conv2D)             (None, 12, 12, 128)  139264      block17_17_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_354 (BatchN (None, 12, 12, 128)  384         conv2d_348[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_348 (Activation)     (None, 12, 12, 128)  0           batch_normalization_354[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_349 (Conv2D)             (None, 12, 12, 160)  143360      activation_348[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_355 (BatchN (None, 12, 12, 160)  480         conv2d_349[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_349 (Activation)     (None, 12, 12, 160)  0           batch_normalization_355[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_347 (Conv2D)             (None, 12, 12, 192)  208896      block17_17_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_350 (Conv2D)             (None, 12, 12, 192)  215040      activation_349[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_353 (BatchN (None, 12, 12, 192)  576         conv2d_347[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_356 (BatchN (None, 12, 12, 192)  576         conv2d_350[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_347 (Activation)     (None, 12, 12, 192)  0           batch_normalization_353[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_350 (Activation)     (None, 12, 12, 192)  0           batch_normalization_356[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_18_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_347[0][0]             \n",
            "                                                                 activation_350[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_18_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_18_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_18 (Lambda)             (None, 12, 12, 1088) 0           block17_17_ac[0][0]              \n",
            "                                                                 block17_18_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_18_ac (Activation)      (None, 12, 12, 1088) 0           block17_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_352 (Conv2D)             (None, 12, 12, 128)  139264      block17_18_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_358 (BatchN (None, 12, 12, 128)  384         conv2d_352[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_352 (Activation)     (None, 12, 12, 128)  0           batch_normalization_358[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_353 (Conv2D)             (None, 12, 12, 160)  143360      activation_352[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_359 (BatchN (None, 12, 12, 160)  480         conv2d_353[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_353 (Activation)     (None, 12, 12, 160)  0           batch_normalization_359[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_351 (Conv2D)             (None, 12, 12, 192)  208896      block17_18_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_354 (Conv2D)             (None, 12, 12, 192)  215040      activation_353[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_357 (BatchN (None, 12, 12, 192)  576         conv2d_351[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_360 (BatchN (None, 12, 12, 192)  576         conv2d_354[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_351 (Activation)     (None, 12, 12, 192)  0           batch_normalization_357[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_354 (Activation)     (None, 12, 12, 192)  0           batch_normalization_360[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_19_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_351[0][0]             \n",
            "                                                                 activation_354[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_19_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_19_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_19 (Lambda)             (None, 12, 12, 1088) 0           block17_18_ac[0][0]              \n",
            "                                                                 block17_19_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_19_ac (Activation)      (None, 12, 12, 1088) 0           block17_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_356 (Conv2D)             (None, 12, 12, 128)  139264      block17_19_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_362 (BatchN (None, 12, 12, 128)  384         conv2d_356[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_356 (Activation)     (None, 12, 12, 128)  0           batch_normalization_362[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_357 (Conv2D)             (None, 12, 12, 160)  143360      activation_356[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_363 (BatchN (None, 12, 12, 160)  480         conv2d_357[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_357 (Activation)     (None, 12, 12, 160)  0           batch_normalization_363[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_355 (Conv2D)             (None, 12, 12, 192)  208896      block17_19_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_358 (Conv2D)             (None, 12, 12, 192)  215040      activation_357[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_361 (BatchN (None, 12, 12, 192)  576         conv2d_355[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_364 (BatchN (None, 12, 12, 192)  576         conv2d_358[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_355 (Activation)     (None, 12, 12, 192)  0           batch_normalization_361[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_358 (Activation)     (None, 12, 12, 192)  0           batch_normalization_364[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_20_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_355[0][0]             \n",
            "                                                                 activation_358[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_20_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_20_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_20 (Lambda)             (None, 12, 12, 1088) 0           block17_19_ac[0][0]              \n",
            "                                                                 block17_20_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_20_ac (Activation)      (None, 12, 12, 1088) 0           block17_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_363 (Conv2D)             (None, 12, 12, 256)  278528      block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_369 (BatchN (None, 12, 12, 256)  768         conv2d_363[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_363 (Activation)     (None, 12, 12, 256)  0           batch_normalization_369[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_359 (Conv2D)             (None, 12, 12, 256)  278528      block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_361 (Conv2D)             (None, 12, 12, 256)  278528      block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_364 (Conv2D)             (None, 12, 12, 288)  663552      activation_363[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_365 (BatchN (None, 12, 12, 256)  768         conv2d_359[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_367 (BatchN (None, 12, 12, 256)  768         conv2d_361[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_370 (BatchN (None, 12, 12, 288)  864         conv2d_364[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_359 (Activation)     (None, 12, 12, 256)  0           batch_normalization_365[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_361 (Activation)     (None, 12, 12, 256)  0           batch_normalization_367[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_364 (Activation)     (None, 12, 12, 288)  0           batch_normalization_370[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_360 (Conv2D)             (None, 5, 5, 384)    884736      activation_359[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_362 (Conv2D)             (None, 5, 5, 288)    663552      activation_361[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_365 (Conv2D)             (None, 5, 5, 320)    829440      activation_364[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_366 (BatchN (None, 5, 5, 384)    1152        conv2d_360[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_368 (BatchN (None, 5, 5, 288)    864         conv2d_362[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_371 (BatchN (None, 5, 5, 320)    960         conv2d_365[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_360 (Activation)     (None, 5, 5, 384)    0           batch_normalization_366[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_362 (Activation)     (None, 5, 5, 288)    0           batch_normalization_368[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_365 (Activation)     (None, 5, 5, 320)    0           batch_normalization_371[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 5, 5, 1088)   0           block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "mixed_7a (Concatenate)          (None, 5, 5, 2080)   0           activation_360[0][0]             \n",
            "                                                                 activation_362[0][0]             \n",
            "                                                                 activation_365[0][0]             \n",
            "                                                                 max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_367 (Conv2D)             (None, 5, 5, 192)    399360      mixed_7a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_373 (BatchN (None, 5, 5, 192)    576         conv2d_367[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_367 (Activation)     (None, 5, 5, 192)    0           batch_normalization_373[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_368 (Conv2D)             (None, 5, 5, 224)    129024      activation_367[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_374 (BatchN (None, 5, 5, 224)    672         conv2d_368[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_368 (Activation)     (None, 5, 5, 224)    0           batch_normalization_374[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_366 (Conv2D)             (None, 5, 5, 192)    399360      mixed_7a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_369 (Conv2D)             (None, 5, 5, 256)    172032      activation_368[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_372 (BatchN (None, 5, 5, 192)    576         conv2d_366[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_375 (BatchN (None, 5, 5, 256)    768         conv2d_369[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_366 (Activation)     (None, 5, 5, 192)    0           batch_normalization_372[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_369 (Activation)     (None, 5, 5, 256)    0           batch_normalization_375[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_1_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_366[0][0]             \n",
            "                                                                 activation_369[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_1_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_1_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_1 (Lambda)               (None, 5, 5, 2080)   0           mixed_7a[0][0]                   \n",
            "                                                                 block8_1_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_1_ac (Activation)        (None, 5, 5, 2080)   0           block8_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_371 (Conv2D)             (None, 5, 5, 192)    399360      block8_1_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_377 (BatchN (None, 5, 5, 192)    576         conv2d_371[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_371 (Activation)     (None, 5, 5, 192)    0           batch_normalization_377[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_372 (Conv2D)             (None, 5, 5, 224)    129024      activation_371[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_378 (BatchN (None, 5, 5, 224)    672         conv2d_372[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_372 (Activation)     (None, 5, 5, 224)    0           batch_normalization_378[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_370 (Conv2D)             (None, 5, 5, 192)    399360      block8_1_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_373 (Conv2D)             (None, 5, 5, 256)    172032      activation_372[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_376 (BatchN (None, 5, 5, 192)    576         conv2d_370[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_379 (BatchN (None, 5, 5, 256)    768         conv2d_373[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_370 (Activation)     (None, 5, 5, 192)    0           batch_normalization_376[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_373 (Activation)     (None, 5, 5, 256)    0           batch_normalization_379[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_2_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_370[0][0]             \n",
            "                                                                 activation_373[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_2_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_2_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_2 (Lambda)               (None, 5, 5, 2080)   0           block8_1_ac[0][0]                \n",
            "                                                                 block8_2_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_2_ac (Activation)        (None, 5, 5, 2080)   0           block8_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_375 (Conv2D)             (None, 5, 5, 192)    399360      block8_2_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_381 (BatchN (None, 5, 5, 192)    576         conv2d_375[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_375 (Activation)     (None, 5, 5, 192)    0           batch_normalization_381[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_376 (Conv2D)             (None, 5, 5, 224)    129024      activation_375[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_382 (BatchN (None, 5, 5, 224)    672         conv2d_376[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_376 (Activation)     (None, 5, 5, 224)    0           batch_normalization_382[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_374 (Conv2D)             (None, 5, 5, 192)    399360      block8_2_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_377 (Conv2D)             (None, 5, 5, 256)    172032      activation_376[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_380 (BatchN (None, 5, 5, 192)    576         conv2d_374[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_383 (BatchN (None, 5, 5, 256)    768         conv2d_377[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_374 (Activation)     (None, 5, 5, 192)    0           batch_normalization_380[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_377 (Activation)     (None, 5, 5, 256)    0           batch_normalization_383[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_3_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_374[0][0]             \n",
            "                                                                 activation_377[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_3_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_3_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_3 (Lambda)               (None, 5, 5, 2080)   0           block8_2_ac[0][0]                \n",
            "                                                                 block8_3_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_3_ac (Activation)        (None, 5, 5, 2080)   0           block8_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_379 (Conv2D)             (None, 5, 5, 192)    399360      block8_3_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_385 (BatchN (None, 5, 5, 192)    576         conv2d_379[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_379 (Activation)     (None, 5, 5, 192)    0           batch_normalization_385[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_380 (Conv2D)             (None, 5, 5, 224)    129024      activation_379[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_386 (BatchN (None, 5, 5, 224)    672         conv2d_380[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_380 (Activation)     (None, 5, 5, 224)    0           batch_normalization_386[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_378 (Conv2D)             (None, 5, 5, 192)    399360      block8_3_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_381 (Conv2D)             (None, 5, 5, 256)    172032      activation_380[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_384 (BatchN (None, 5, 5, 192)    576         conv2d_378[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_387 (BatchN (None, 5, 5, 256)    768         conv2d_381[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_378 (Activation)     (None, 5, 5, 192)    0           batch_normalization_384[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_381 (Activation)     (None, 5, 5, 256)    0           batch_normalization_387[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_4_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_378[0][0]             \n",
            "                                                                 activation_381[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_4_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_4_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_4 (Lambda)               (None, 5, 5, 2080)   0           block8_3_ac[0][0]                \n",
            "                                                                 block8_4_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_4_ac (Activation)        (None, 5, 5, 2080)   0           block8_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_383 (Conv2D)             (None, 5, 5, 192)    399360      block8_4_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_389 (BatchN (None, 5, 5, 192)    576         conv2d_383[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_383 (Activation)     (None, 5, 5, 192)    0           batch_normalization_389[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_384 (Conv2D)             (None, 5, 5, 224)    129024      activation_383[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_390 (BatchN (None, 5, 5, 224)    672         conv2d_384[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_384 (Activation)     (None, 5, 5, 224)    0           batch_normalization_390[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_382 (Conv2D)             (None, 5, 5, 192)    399360      block8_4_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_385 (Conv2D)             (None, 5, 5, 256)    172032      activation_384[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_388 (BatchN (None, 5, 5, 192)    576         conv2d_382[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_391 (BatchN (None, 5, 5, 256)    768         conv2d_385[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_382 (Activation)     (None, 5, 5, 192)    0           batch_normalization_388[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_385 (Activation)     (None, 5, 5, 256)    0           batch_normalization_391[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_5_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_382[0][0]             \n",
            "                                                                 activation_385[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_5_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_5_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_5 (Lambda)               (None, 5, 5, 2080)   0           block8_4_ac[0][0]                \n",
            "                                                                 block8_5_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_5_ac (Activation)        (None, 5, 5, 2080)   0           block8_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_387 (Conv2D)             (None, 5, 5, 192)    399360      block8_5_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_393 (BatchN (None, 5, 5, 192)    576         conv2d_387[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_387 (Activation)     (None, 5, 5, 192)    0           batch_normalization_393[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_388 (Conv2D)             (None, 5, 5, 224)    129024      activation_387[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_394 (BatchN (None, 5, 5, 224)    672         conv2d_388[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_388 (Activation)     (None, 5, 5, 224)    0           batch_normalization_394[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_386 (Conv2D)             (None, 5, 5, 192)    399360      block8_5_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_389 (Conv2D)             (None, 5, 5, 256)    172032      activation_388[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_392 (BatchN (None, 5, 5, 192)    576         conv2d_386[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_395 (BatchN (None, 5, 5, 256)    768         conv2d_389[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_386 (Activation)     (None, 5, 5, 192)    0           batch_normalization_392[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_389 (Activation)     (None, 5, 5, 256)    0           batch_normalization_395[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_6_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_386[0][0]             \n",
            "                                                                 activation_389[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_6_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_6_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_6 (Lambda)               (None, 5, 5, 2080)   0           block8_5_ac[0][0]                \n",
            "                                                                 block8_6_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_6_ac (Activation)        (None, 5, 5, 2080)   0           block8_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_391 (Conv2D)             (None, 5, 5, 192)    399360      block8_6_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_397 (BatchN (None, 5, 5, 192)    576         conv2d_391[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_391 (Activation)     (None, 5, 5, 192)    0           batch_normalization_397[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_392 (Conv2D)             (None, 5, 5, 224)    129024      activation_391[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_398 (BatchN (None, 5, 5, 224)    672         conv2d_392[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_392 (Activation)     (None, 5, 5, 224)    0           batch_normalization_398[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_390 (Conv2D)             (None, 5, 5, 192)    399360      block8_6_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_393 (Conv2D)             (None, 5, 5, 256)    172032      activation_392[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_396 (BatchN (None, 5, 5, 192)    576         conv2d_390[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_399 (BatchN (None, 5, 5, 256)    768         conv2d_393[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_390 (Activation)     (None, 5, 5, 192)    0           batch_normalization_396[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_393 (Activation)     (None, 5, 5, 256)    0           batch_normalization_399[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_7_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_390[0][0]             \n",
            "                                                                 activation_393[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_7_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_7_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_7 (Lambda)               (None, 5, 5, 2080)   0           block8_6_ac[0][0]                \n",
            "                                                                 block8_7_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_7_ac (Activation)        (None, 5, 5, 2080)   0           block8_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_395 (Conv2D)             (None, 5, 5, 192)    399360      block8_7_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_401 (BatchN (None, 5, 5, 192)    576         conv2d_395[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_395 (Activation)     (None, 5, 5, 192)    0           batch_normalization_401[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_396 (Conv2D)             (None, 5, 5, 224)    129024      activation_395[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_402 (BatchN (None, 5, 5, 224)    672         conv2d_396[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_396 (Activation)     (None, 5, 5, 224)    0           batch_normalization_402[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_394 (Conv2D)             (None, 5, 5, 192)    399360      block8_7_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_397 (Conv2D)             (None, 5, 5, 256)    172032      activation_396[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_400 (BatchN (None, 5, 5, 192)    576         conv2d_394[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_403 (BatchN (None, 5, 5, 256)    768         conv2d_397[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_394 (Activation)     (None, 5, 5, 192)    0           batch_normalization_400[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_397 (Activation)     (None, 5, 5, 256)    0           batch_normalization_403[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_8_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_394[0][0]             \n",
            "                                                                 activation_397[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_8_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_8_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_8 (Lambda)               (None, 5, 5, 2080)   0           block8_7_ac[0][0]                \n",
            "                                                                 block8_8_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_8_ac (Activation)        (None, 5, 5, 2080)   0           block8_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_399 (Conv2D)             (None, 5, 5, 192)    399360      block8_8_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_405 (BatchN (None, 5, 5, 192)    576         conv2d_399[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_399 (Activation)     (None, 5, 5, 192)    0           batch_normalization_405[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_400 (Conv2D)             (None, 5, 5, 224)    129024      activation_399[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_406 (BatchN (None, 5, 5, 224)    672         conv2d_400[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_400 (Activation)     (None, 5, 5, 224)    0           batch_normalization_406[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_398 (Conv2D)             (None, 5, 5, 192)    399360      block8_8_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_401 (Conv2D)             (None, 5, 5, 256)    172032      activation_400[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_404 (BatchN (None, 5, 5, 192)    576         conv2d_398[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_407 (BatchN (None, 5, 5, 256)    768         conv2d_401[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_398 (Activation)     (None, 5, 5, 192)    0           batch_normalization_404[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_401 (Activation)     (None, 5, 5, 256)    0           batch_normalization_407[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_9_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_398[0][0]             \n",
            "                                                                 activation_401[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_9_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_9_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_9 (Lambda)               (None, 5, 5, 2080)   0           block8_8_ac[0][0]                \n",
            "                                                                 block8_9_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_9_ac (Activation)        (None, 5, 5, 2080)   0           block8_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_403 (Conv2D)             (None, 5, 5, 192)    399360      block8_9_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_409 (BatchN (None, 5, 5, 192)    576         conv2d_403[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_403 (Activation)     (None, 5, 5, 192)    0           batch_normalization_409[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_404 (Conv2D)             (None, 5, 5, 224)    129024      activation_403[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_410 (BatchN (None, 5, 5, 224)    672         conv2d_404[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_404 (Activation)     (None, 5, 5, 224)    0           batch_normalization_410[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_402 (Conv2D)             (None, 5, 5, 192)    399360      block8_9_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_405 (Conv2D)             (None, 5, 5, 256)    172032      activation_404[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_408 (BatchN (None, 5, 5, 192)    576         conv2d_402[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_411 (BatchN (None, 5, 5, 256)    768         conv2d_405[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_402 (Activation)     (None, 5, 5, 192)    0           batch_normalization_408[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_405 (Activation)     (None, 5, 5, 256)    0           batch_normalization_411[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_10_mixed (Concatenate)   (None, 5, 5, 448)    0           activation_402[0][0]             \n",
            "                                                                 activation_405[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_10_conv (Conv2D)         (None, 5, 5, 2080)   933920      block8_10_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_10 (Lambda)              (None, 5, 5, 2080)   0           block8_9_ac[0][0]                \n",
            "                                                                 block8_10_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_7b (Conv2D)                (None, 5, 5, 1536)   3194880     block8_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_7b_bn (BatchNormalization) (None, 5, 5, 1536)   4608        conv_7b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_7b_ac (Activation)         (None, 5, 5, 1536)   0           conv_7b_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 1536)         0           conv_7b_ac[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 54,336,736\n",
            "Trainable params: 0\n",
            "Non-trainable params: 54,336,736\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mO_AeE-3tKtA",
        "colab": {}
      },
      "source": [
        "'''Flatten -> is the function that converts the pooled feature map to a single column that is passed to the fully connected layer.\n",
        "Dense -> adds the fully connected layer to the neural network.\n",
        "Dropout->regularization\n",
        "BatchNormalization-> sends data(Images) as the batches\n",
        "\n",
        "Model -> Model groups layers into an object with training and inference features.\n",
        "Sequential -> Sequential groups a linear stack of layers into a tf.keras.Model.\n",
        "\n",
        "to_categorical -> Converts a class vector (integers) to binary class matrix.\n",
        "\n",
        "Conv2D -> This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs\n",
        "MaxPooling2D -> Downsamples the input representation by taking the maximum value over the window defined by pool_size for each dimension along the features axis\n",
        "\n",
        "ReduceLROnPlateau -> Reduce learning rate when a metric(accuracy) has stopped improving.\n",
        "'''\n",
        "from tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_czPjK7StKnQ",
        "colab": {}
      },
      "source": [
        "model=Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KbVmVxJpvVWQ",
        "colab": {}
      },
      "source": [
        "model.add(Dense(8,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7EIaAGUtuVx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1e98b433-4f55-4cef-9325-6f7df634d38a"
      },
      "source": [
        "'''\n",
        "Adam -> Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments\n",
        "SGD -> Stochastic gradient descent and momentum optimizer.\n",
        "Adagrad -> Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training\n",
        "Adadelta -> Adadelta optimization is a stochastic gradient descent method that is based on adaptive learning rate per dimension\n",
        "RMSprop -> maintain a moving (discounted) average of the square of gradients\n",
        "           divide gradient by the root of this average\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAdam -> Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments\\nSGD -> Stochastic gradient descent and momentum optimizer.\\nAdagrad -> Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training\\nAdadelta -> Adadelta optimization is a stochastic gradient descent method that is based on adaptive learning rate per dimension\\nRMSprop -> maintain a moving (discounted) average of the square of gradients\\n           divide gradient by the root of this average\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E7e0rs5BvYV6",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
        "reduce_learning_rate = ReduceLROnPlateau(monitor='loss',\n",
        "                                         factor=0.1,\n",
        "                                         patience=2,\n",
        "                                         cooldown=2,\n",
        "                                         min_lr=0.00001,\n",
        "                                         verbose=1)\n",
        "callbacks = [reduce_learning_rate]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdDQTSmVtuV6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ce447d07-42bc-4df3-9fac-6433dddb9195"
      },
      "source": [
        "''''\n",
        "compile -> Configures the model for training\n",
        "evaluate -> Returns the loss value & metrics values for the model in test mode\n",
        "            Computation is done in batches.\n",
        "evaluate_generator -> Evaluates the model on a data generator.\n",
        "fit -> Trains the model for a fixed number of epochs (iterations on a dataset).\n",
        "fit_generator -> Fits the model on data yielded batch-by-batch by a Python generator.\n",
        "predict -> Generates output predictions for the input samples.\n",
        "           Computation is done in batches.\n",
        "summary -> Prints a string summary of the network.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"'\\ncompile -> Configures the model for training\\nevaluate -> Returns the loss value & metrics values for the model in test mode\\n            Computation is done in batches.\\nevaluate_generator -> Evaluates the model on a data generator.\\nfit -> Trains the model for a fixed number of epochs (iterations on a dataset).\\nfit_generator -> Fits the model on data yielded batch-by-batch by a Python generator.\\npredict -> Generates output predictions for the input samples.\\n           Computation is done in batches.\\nsummary -> Prints a string summary of the network.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZnMmanWHvgTB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "bf049265-9472-4ead-a0dc-af7f3a1744c1"
      },
      "source": [
        "model.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_resnet_v2 (Model)  (None, 1536)              54336736  \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 1536)              0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 1536)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_412 (Bat (None, 1536)              6144      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 512)               786944    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_413 (Bat (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_414 (Bat (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 8)                 1032      \n",
            "=================================================================\n",
            "Total params: 55,199,080\n",
            "Trainable params: 857,992\n",
            "Non-trainable params: 54,341,088\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LWXUEVcbvkSC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a11f022-f2a1-4a79-f25b-5be6df56d38d"
      },
      "source": [
        "model.fit(train_datagen.flow(x_train, to_categorical(y_train,8), batch_size=16),epochs=50,callbacks=callbacks,\n",
        "          validation_data= valid_datagen.flow(x_valid, to_categorical(y_valid,8), batch_size=16),verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "22/22 [==============================] - 9s 389ms/step - loss: 2.3884 - accuracy: 0.1971 - val_loss: 1.9726 - val_accuracy: 0.3158 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 4s 194ms/step - loss: 1.8817 - accuracy: 0.3855 - val_loss: 1.6582 - val_accuracy: 0.4211 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 4s 194ms/step - loss: 1.7790 - accuracy: 0.4290 - val_loss: 1.3398 - val_accuracy: 0.5789 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 4s 194ms/step - loss: 1.5867 - accuracy: 0.4580 - val_loss: 1.3927 - val_accuracy: 0.5789 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 4s 196ms/step - loss: 1.4664 - accuracy: 0.4464 - val_loss: 1.2822 - val_accuracy: 0.5263 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 4s 198ms/step - loss: 1.4911 - accuracy: 0.4812 - val_loss: 1.1231 - val_accuracy: 0.5789 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 1.3097 - accuracy: 0.5449 - val_loss: 1.1638 - val_accuracy: 0.6316 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 1.2789 - accuracy: 0.5304 - val_loss: 1.2446 - val_accuracy: 0.5789 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 4s 194ms/step - loss: 1.2599 - accuracy: 0.5623 - val_loss: 1.2356 - val_accuracy: 0.5789 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 4s 194ms/step - loss: 1.2350 - accuracy: 0.5739 - val_loss: 1.1405 - val_accuracy: 0.5789 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 4s 194ms/step - loss: 1.1629 - accuracy: 0.5739 - val_loss: 1.1020 - val_accuracy: 0.6316 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 1.2939 - accuracy: 0.5362 - val_loss: 1.0698 - val_accuracy: 0.6316 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.2261 - accuracy: 0.5507\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 1.2261 - accuracy: 0.5507 - val_loss: 1.1144 - val_accuracy: 0.5789 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 4s 196ms/step - loss: 1.1149 - accuracy: 0.5971 - val_loss: 1.1036 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 4s 198ms/step - loss: 1.0829 - accuracy: 0.6232 - val_loss: 1.1065 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 4s 196ms/step - loss: 1.0447 - accuracy: 0.6145 - val_loss: 1.1101 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 4s 194ms/step - loss: 1.0421 - accuracy: 0.6261 - val_loss: 1.0885 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 1.0700 - accuracy: 0.6058 - val_loss: 1.0938 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.1284 - accuracy: 0.5710\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 1.1284 - accuracy: 0.5710 - val_loss: 1.0978 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 4s 200ms/step - loss: 1.0901 - accuracy: 0.6029 - val_loss: 1.0947 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 4s 198ms/step - loss: 0.9808 - accuracy: 0.6580 - val_loss: 1.0951 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 4s 203ms/step - loss: 1.0429 - accuracy: 0.6290 - val_loss: 1.0930 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0668 - accuracy: 0.6464\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 1.0668 - accuracy: 0.6464 - val_loss: 1.0913 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 1.1267 - accuracy: 0.6145 - val_loss: 1.0946 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 1.0928 - accuracy: 0.6058 - val_loss: 1.0907 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 4s 194ms/step - loss: 1.0048 - accuracy: 0.6667 - val_loss: 1.0844 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 4s 196ms/step - loss: 0.9855 - accuracy: 0.6406 - val_loss: 1.0848 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 1.0744 - accuracy: 0.6116 - val_loss: 1.0892 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 4s 194ms/step - loss: 1.0572 - accuracy: 0.6232 - val_loss: 1.0956 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 4s 192ms/step - loss: 0.9670 - accuracy: 0.6812 - val_loss: 1.1001 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 4s 193ms/step - loss: 1.0527 - accuracy: 0.6232 - val_loss: 1.0943 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 0.9831 - accuracy: 0.6667 - val_loss: 1.0889 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 1.1026 - accuracy: 0.6174 - val_loss: 1.0823 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 1.1266 - accuracy: 0.5768 - val_loss: 1.0774 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 1.0521 - accuracy: 0.6522 - val_loss: 1.0799 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 4s 194ms/step - loss: 1.1089 - accuracy: 0.5971 - val_loss: 1.0832 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 4s 194ms/step - loss: 1.0904 - accuracy: 0.6319 - val_loss: 1.0885 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 4s 196ms/step - loss: 1.1138 - accuracy: 0.6116 - val_loss: 1.0893 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 4s 196ms/step - loss: 1.0538 - accuracy: 0.6348 - val_loss: 1.0843 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 1.0686 - accuracy: 0.6406 - val_loss: 1.0815 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 4s 196ms/step - loss: 1.0571 - accuracy: 0.6290 - val_loss: 1.0780 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 4s 196ms/step - loss: 1.0618 - accuracy: 0.6029 - val_loss: 1.0785 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 4s 191ms/step - loss: 1.0072 - accuracy: 0.6319 - val_loss: 1.0839 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 1.1017 - accuracy: 0.6116 - val_loss: 1.0855 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 1.1110 - accuracy: 0.6145 - val_loss: 1.0791 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 1.0420 - accuracy: 0.5942 - val_loss: 1.0784 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 4s 193ms/step - loss: 1.0813 - accuracy: 0.6000 - val_loss: 1.0813 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 4s 193ms/step - loss: 1.0095 - accuracy: 0.6609 - val_loss: 1.0968 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 4s 196ms/step - loss: 1.0712 - accuracy: 0.6174 - val_loss: 1.0940 - val_accuracy: 0.6316 - lr: 1.0000e-05\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 4s 192ms/step - loss: 1.0873 - accuracy: 0.5913 - val_loss: 1.1070 - val_accuracy: 0.6316 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f86e7e49cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpDwmXyhEUij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label1 = model.predict(test_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOmS2CLavQnC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "370d6929-4254-49dc-f3d8-44abe0371bf8"
      },
      "source": [
        "labels = model.predict(test_img)\n",
        "label = [np.argmax(i) for i in labels]\n",
        "class_label = [inverse_map[x] for x in label]\n",
        "submission = pd.DataFrame({ 'Image': test.Image, 'target': class_label })\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>508.jpg</td>\n",
              "      <td>bharatanatyam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>246.jpg</td>\n",
              "      <td>bharatanatyam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>473.jpg</td>\n",
              "      <td>bharatanatyam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>485.jpg</td>\n",
              "      <td>bharatanatyam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128.jpg</td>\n",
              "      <td>bharatanatyam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Image         target\n",
              "0  508.jpg  bharatanatyam\n",
              "1  246.jpg  bharatanatyam\n",
              "2  473.jpg  bharatanatyam\n",
              "3  485.jpg  bharatanatyam\n",
              "4  128.jpg  bharatanatyam"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0shzsDEYvQri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv('/content/drive/My Drive/dataset/inception.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aetn4kALtuWK",
        "colab_type": "text"
      },
      "source": [
        "## ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zUIQqCMPvuNq",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "base_model_2= ResNet50(include_top=False, weights='imagenet',input_shape=(img_h,img_w,3), pooling='max')\n",
        "'''for layer in base_model_2.layers[:-3]:\n",
        "    layer.trainable=False'''\n",
        "base_model_2.trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9zc7uQ5UwZ_i",
        "colab": {}
      },
      "source": [
        "    \n",
        "model_2=Sequential()\n",
        "model_2.add(base_model_2)\n",
        "\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dropout(0.4))\n",
        "model_2.add(BatchNormalization())\n",
        "\n",
        "model_2.add(Dense(512, activation='relu'))\n",
        "model_2.add(Dropout(0.2))\n",
        "model_2.add(BatchNormalization())\n",
        "\n",
        "model_2.add(Dense(128, activation='relu'))\n",
        "model_2.add(Dropout(0.1))\n",
        "model_2.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model_2.add(Dense(8,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JJiQjDBYwdDh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "82e1e381-028d-4517-bfc1-f16b212d42b1"
      },
      "source": [
        "model_2.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Model)             (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_415 (Bat (None, 2048)              8192      \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_416 (Bat (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_417 (Bat (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 8)                 1032      \n",
            "=================================================================\n",
            "Total params: 24,714,248\n",
            "Trainable params: 1,121,160\n",
            "Non-trainable params: 23,593,088\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XrJLAmOjwf7R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09994ab3-5e71-4e6a-e7c7-c1c2aa2d4e91"
      },
      "source": [
        "model_2.fit(      train_datagen.flow(x_train, to_categorical(y_train,8), batch_size=32),epochs=50,callbacks=callbacks,\n",
        "validation_data= valid_datagen.flow(x_valid, to_categorical(y_valid,8), batch_size=32),verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "11/11 [==============================] - 5s 481ms/step - loss: 2.7221 - accuracy: 0.1391 - val_loss: 1.9050 - val_accuracy: 0.2632 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "11/11 [==============================] - 4s 328ms/step - loss: 2.6184 - accuracy: 0.1797 - val_loss: 1.9471 - val_accuracy: 0.3158 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "11/11 [==============================] - 4s 335ms/step - loss: 2.4962 - accuracy: 0.1913 - val_loss: 1.8900 - val_accuracy: 0.3158 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "11/11 [==============================] - 4s 331ms/step - loss: 2.2947 - accuracy: 0.2377 - val_loss: 1.8331 - val_accuracy: 0.3684 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "11/11 [==============================] - 4s 345ms/step - loss: 2.3931 - accuracy: 0.1826 - val_loss: 1.7309 - val_accuracy: 0.4211 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "11/11 [==============================] - ETA: 0s - loss: 2.3443 - accuracy: 0.2087\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "11/11 [==============================] - 4s 334ms/step - loss: 2.3443 - accuracy: 0.2087 - val_loss: 1.7527 - val_accuracy: 0.3684 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "11/11 [==============================] - 4s 328ms/step - loss: 2.3250 - accuracy: 0.2116 - val_loss: 1.7712 - val_accuracy: 0.3684 - lr: 1.0000e-04\n",
            "Epoch 8/50\n",
            "11/11 [==============================] - 4s 329ms/step - loss: 2.2733 - accuracy: 0.1913 - val_loss: 1.7805 - val_accuracy: 0.3158 - lr: 1.0000e-04\n",
            "Epoch 9/50\n",
            "11/11 [==============================] - 4s 336ms/step - loss: 2.2378 - accuracy: 0.2435 - val_loss: 1.7830 - val_accuracy: 0.3158 - lr: 1.0000e-04\n",
            "Epoch 10/50\n",
            "11/11 [==============================] - 4s 332ms/step - loss: 2.1357 - accuracy: 0.2348 - val_loss: 1.7857 - val_accuracy: 0.3158 - lr: 1.0000e-04\n",
            "Epoch 11/50\n",
            "11/11 [==============================] - 4s 329ms/step - loss: 2.1722 - accuracy: 0.2609 - val_loss: 1.7876 - val_accuracy: 0.3158 - lr: 1.0000e-04\n",
            "Epoch 12/50\n",
            "11/11 [==============================] - ETA: 0s - loss: 2.2628 - accuracy: 0.1942\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "11/11 [==============================] - 4s 330ms/step - loss: 2.2628 - accuracy: 0.1942 - val_loss: 1.7861 - val_accuracy: 0.3158 - lr: 1.0000e-04\n",
            "Epoch 13/50\n",
            "11/11 [==============================] - 4s 333ms/step - loss: 2.2198 - accuracy: 0.2290 - val_loss: 1.7808 - val_accuracy: 0.3684 - lr: 1.0000e-05\n",
            "Epoch 14/50\n",
            "11/11 [==============================] - 4s 328ms/step - loss: 2.2977 - accuracy: 0.2232 - val_loss: 1.7696 - val_accuracy: 0.3158 - lr: 1.0000e-05\n",
            "Epoch 15/50\n",
            "11/11 [==============================] - 4s 334ms/step - loss: 2.1350 - accuracy: 0.2667 - val_loss: 1.7585 - val_accuracy: 0.3158 - lr: 1.0000e-05\n",
            "Epoch 16/50\n",
            "11/11 [==============================] - 4s 338ms/step - loss: 2.2399 - accuracy: 0.2377 - val_loss: 1.7450 - val_accuracy: 0.3684 - lr: 1.0000e-05\n",
            "Epoch 17/50\n",
            "11/11 [==============================] - ETA: 0s - loss: 2.1698 - accuracy: 0.2696\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "11/11 [==============================] - 4s 333ms/step - loss: 2.1698 - accuracy: 0.2696 - val_loss: 1.7308 - val_accuracy: 0.3684 - lr: 1.0000e-05\n",
            "Epoch 18/50\n",
            "11/11 [==============================] - 4s 332ms/step - loss: 2.1949 - accuracy: 0.2406 - val_loss: 1.7187 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 19/50\n",
            "11/11 [==============================] - 4s 333ms/step - loss: 2.1392 - accuracy: 0.2406 - val_loss: 1.7136 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 20/50\n",
            "11/11 [==============================] - 4s 328ms/step - loss: 2.1697 - accuracy: 0.2667 - val_loss: 1.7045 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 21/50\n",
            "11/11 [==============================] - 4s 327ms/step - loss: 2.1505 - accuracy: 0.2174 - val_loss: 1.6981 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 22/50\n",
            "11/11 [==============================] - 4s 321ms/step - loss: 2.1681 - accuracy: 0.2261 - val_loss: 1.6935 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 23/50\n",
            "11/11 [==============================] - 4s 325ms/step - loss: 2.2247 - accuracy: 0.1942 - val_loss: 1.6906 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 24/50\n",
            "11/11 [==============================] - 4s 328ms/step - loss: 2.2663 - accuracy: 0.2232 - val_loss: 1.6841 - val_accuracy: 0.4737 - lr: 1.0000e-05\n",
            "Epoch 25/50\n",
            "11/11 [==============================] - 4s 336ms/step - loss: 2.3189 - accuracy: 0.1942 - val_loss: 1.6780 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 26/50\n",
            "11/11 [==============================] - 4s 323ms/step - loss: 2.2249 - accuracy: 0.2203 - val_loss: 1.6775 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 27/50\n",
            "11/11 [==============================] - 4s 322ms/step - loss: 2.1655 - accuracy: 0.2464 - val_loss: 1.6750 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 28/50\n",
            "11/11 [==============================] - 3s 315ms/step - loss: 2.1442 - accuracy: 0.2493 - val_loss: 1.6689 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 29/50\n",
            "11/11 [==============================] - 4s 323ms/step - loss: 2.2655 - accuracy: 0.2377 - val_loss: 1.6608 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 30/50\n",
            "11/11 [==============================] - 4s 320ms/step - loss: 2.1550 - accuracy: 0.2406 - val_loss: 1.6579 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 31/50\n",
            "11/11 [==============================] - 4s 319ms/step - loss: 2.2433 - accuracy: 0.2290 - val_loss: 1.6516 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 32/50\n",
            "11/11 [==============================] - 4s 323ms/step - loss: 2.1534 - accuracy: 0.2319 - val_loss: 1.6422 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 33/50\n",
            "11/11 [==============================] - 4s 322ms/step - loss: 2.0957 - accuracy: 0.2464 - val_loss: 1.6418 - val_accuracy: 0.3684 - lr: 1.0000e-05\n",
            "Epoch 34/50\n",
            "11/11 [==============================] - 4s 321ms/step - loss: 2.1647 - accuracy: 0.2145 - val_loss: 1.6436 - val_accuracy: 0.3684 - lr: 1.0000e-05\n",
            "Epoch 35/50\n",
            "11/11 [==============================] - 4s 324ms/step - loss: 2.1829 - accuracy: 0.2029 - val_loss: 1.6424 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 36/50\n",
            "11/11 [==============================] - 4s 325ms/step - loss: 2.2196 - accuracy: 0.2203 - val_loss: 1.6389 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 37/50\n",
            "11/11 [==============================] - 4s 323ms/step - loss: 2.2153 - accuracy: 0.2087 - val_loss: 1.6305 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 38/50\n",
            "11/11 [==============================] - 4s 328ms/step - loss: 2.1855 - accuracy: 0.2551 - val_loss: 1.6291 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 39/50\n",
            "11/11 [==============================] - 4s 319ms/step - loss: 2.1958 - accuracy: 0.2203 - val_loss: 1.6272 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 40/50\n",
            "11/11 [==============================] - 4s 328ms/step - loss: 2.1997 - accuracy: 0.2000 - val_loss: 1.6256 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 41/50\n",
            "11/11 [==============================] - 4s 320ms/step - loss: 2.1287 - accuracy: 0.2522 - val_loss: 1.6224 - val_accuracy: 0.4737 - lr: 1.0000e-05\n",
            "Epoch 42/50\n",
            "11/11 [==============================] - 3s 315ms/step - loss: 2.2342 - accuracy: 0.2000 - val_loss: 1.6221 - val_accuracy: 0.4737 - lr: 1.0000e-05\n",
            "Epoch 43/50\n",
            "11/11 [==============================] - 4s 319ms/step - loss: 2.2279 - accuracy: 0.2174 - val_loss: 1.6175 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 44/50\n",
            "11/11 [==============================] - 3s 316ms/step - loss: 2.0946 - accuracy: 0.2319 - val_loss: 1.6184 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 45/50\n",
            "11/11 [==============================] - 4s 322ms/step - loss: 2.1940 - accuracy: 0.2261 - val_loss: 1.6199 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 46/50\n",
            "11/11 [==============================] - 4s 329ms/step - loss: 2.1346 - accuracy: 0.2116 - val_loss: 1.6200 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 47/50\n",
            "11/11 [==============================] - 4s 322ms/step - loss: 2.1910 - accuracy: 0.2580 - val_loss: 1.6213 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 48/50\n",
            "11/11 [==============================] - 4s 329ms/step - loss: 2.2479 - accuracy: 0.2290 - val_loss: 1.6128 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 49/50\n",
            "11/11 [==============================] - 4s 323ms/step - loss: 2.1565 - accuracy: 0.2580 - val_loss: 1.6175 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 50/50\n",
            "11/11 [==============================] - 3s 317ms/step - loss: 2.1425 - accuracy: 0.2203 - val_loss: 1.6174 - val_accuracy: 0.4737 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f86e1705b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3NxQ2kqEjgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label2 = model_2.predict(test_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbEgb90SxaeZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "0a0541cd-e2c7-4461-b73e-80df233289e8"
      },
      "source": [
        "labels = model_2.predict(test_img)\n",
        "label = [np.argmax(i) for i in labels]\n",
        "class_label = [inverse_map[x] for x in label]\n",
        "submission = pd.DataFrame({ 'Image': test.Image, 'target': class_label })\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>508.jpg</td>\n",
              "      <td>mohiniyattam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>246.jpg</td>\n",
              "      <td>mohiniyattam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>473.jpg</td>\n",
              "      <td>mohiniyattam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>485.jpg</td>\n",
              "      <td>mohiniyattam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128.jpg</td>\n",
              "      <td>mohiniyattam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Image        target\n",
              "0  508.jpg  mohiniyattam\n",
              "1  246.jpg  mohiniyattam\n",
              "2  473.jpg  mohiniyattam\n",
              "3  485.jpg  mohiniyattam\n",
              "4  128.jpg  mohiniyattam"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE-035jqxabj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv('/content/drive/My Drive/dataset/resnet50.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU4aBqpTtuWY",
        "colab_type": "text"
      },
      "source": [
        "## VGG19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YkGjFAkcwkJ5",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.vgg19 import VGG19,preprocess_input\n",
        "base_model_3=VGG19(include_top=False, weights='imagenet',input_shape=(img_h,img_w,3), pooling='max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gp64fqhnw_u6",
        "colab": {}
      },
      "source": [
        "for layer in base_model_3.layers[:-4]:\n",
        "    layer.trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYzSZ6N2-JLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vgg19-3\n",
        "model_3=Sequential()\n",
        "model_3.add(base_model_3)\n",
        "model_3.add(Flatten())\n",
        "#model_3.add(BatchNormalization())\n",
        "#model_3.add(Dropout(0.2))\n",
        "model_3.add(Dense(512, activation='relu'))\n",
        "#model_3.add(BatchNormalization())\n",
        "model_3.add(Dropout(0.2))\n",
        "model_3.add(Dense(128, activation='relu'))\n",
        "#model_3.add(BatchNormalization())\n",
        "model_3.add(Dense(8,activation='softmax'))\n",
        "\n",
        "model_3.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "#model_3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDp-kzp7-SZ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04ae8152-1372-4df0-b51f-92990ec1d260"
      },
      "source": [
        "model_3.fit(train_datagen.flow(x_train, to_categorical(y_train,8), batch_size=16),epochs=30,callbacks=callbacks,\n",
        "          validation_data= valid_datagen.flow(x_valid, to_categorical(y_valid,8), batch_size=16),verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "22/22 [==============================] - 5s 207ms/step - loss: 2.1345 - accuracy: 0.1942 - val_loss: 1.8601 - val_accuracy: 0.3684 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 4s 198ms/step - loss: 1.7919 - accuracy: 0.2899 - val_loss: 1.2241 - val_accuracy: 0.5263 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 1.4760 - accuracy: 0.4464 - val_loss: 0.9595 - val_accuracy: 0.6316 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 1.3952 - accuracy: 0.5420 - val_loss: 0.8612 - val_accuracy: 0.6316 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 4s 200ms/step - loss: 1.1729 - accuracy: 0.6000 - val_loss: 1.3872 - val_accuracy: 0.6316 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 4s 201ms/step - loss: 1.1628 - accuracy: 0.5855 - val_loss: 0.5524 - val_accuracy: 0.8947 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 4s 199ms/step - loss: 0.9143 - accuracy: 0.7188 - val_loss: 0.5157 - val_accuracy: 0.7895 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 0.7190 - accuracy: 0.7362 - val_loss: 0.3834 - val_accuracy: 0.8947 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 4s 198ms/step - loss: 0.9168 - accuracy: 0.6957 - val_loss: 0.5928 - val_accuracy: 0.7368 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.7646 - accuracy: 0.7449\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 0.7646 - accuracy: 0.7449 - val_loss: 0.3071 - val_accuracy: 0.9474 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 0.5161 - accuracy: 0.8232 - val_loss: 0.4285 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 4s 198ms/step - loss: 0.4813 - accuracy: 0.8435 - val_loss: 0.4021 - val_accuracy: 0.8421 - lr: 1.0000e-04\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 4s 198ms/step - loss: 0.4123 - accuracy: 0.8696 - val_loss: 0.3585 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 4s 196ms/step - loss: 0.3965 - accuracy: 0.8667 - val_loss: 0.3676 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 4s 198ms/step - loss: 0.3663 - accuracy: 0.8638 - val_loss: 0.3643 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 4s 199ms/step - loss: 0.3903 - accuracy: 0.8348 - val_loss: 0.3901 - val_accuracy: 0.8421 - lr: 1.0000e-04\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 0.3598 - accuracy: 0.8754 - val_loss: 0.3440 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 0.3322 - accuracy: 0.8812 - val_loss: 0.2593 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 4s 199ms/step - loss: 0.3152 - accuracy: 0.8986 - val_loss: 0.3247 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 0.3377 - accuracy: 0.8754 - val_loss: 0.3540 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 4s 195ms/step - loss: 0.3007 - accuracy: 0.9188 - val_loss: 0.3511 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 0.2615 - accuracy: 0.9072 - val_loss: 0.3934 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 4s 203ms/step - loss: 0.2904 - accuracy: 0.8928 - val_loss: 0.4491 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9043\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "22/22 [==============================] - 4s 202ms/step - loss: 0.2655 - accuracy: 0.9043 - val_loss: 0.4235 - val_accuracy: 0.8421 - lr: 1.0000e-04\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 4s 202ms/step - loss: 0.2610 - accuracy: 0.9014 - val_loss: 0.4202 - val_accuracy: 0.8421 - lr: 1.0000e-05\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 4s 201ms/step - loss: 0.2394 - accuracy: 0.9246 - val_loss: 0.4135 - val_accuracy: 0.8421 - lr: 1.0000e-05\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 4s 199ms/step - loss: 0.2363 - accuracy: 0.9101 - val_loss: 0.3995 - val_accuracy: 0.8421 - lr: 1.0000e-05\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 0.2433 - accuracy: 0.9188 - val_loss: 0.4039 - val_accuracy: 0.8421 - lr: 1.0000e-05\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.9101\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "22/22 [==============================] - 4s 198ms/step - loss: 0.2403 - accuracy: 0.9101 - val_loss: 0.4078 - val_accuracy: 0.8421 - lr: 1.0000e-05\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 4s 197ms/step - loss: 0.2742 - accuracy: 0.9072 - val_loss: 0.4080 - val_accuracy: 0.8421 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f86d2630c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inQYCzDFE8lU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label3 = model_3.predict(test_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH5fEXIg0JtN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "35fd928a-d12d-4232-9209-47fcc7e8bfe7"
      },
      "source": [
        "labels = model_3.predict(test_img)\n",
        "label = [np.argmax(i) for i in labels]\n",
        "class_label = [inverse_map[x] for x in label]\n",
        "submission = pd.DataFrame({ 'Image': test.Image, 'target': class_label })\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>508.jpg</td>\n",
              "      <td>odissi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>246.jpg</td>\n",
              "      <td>mohiniyattam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>473.jpg</td>\n",
              "      <td>odissi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>485.jpg</td>\n",
              "      <td>odissi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128.jpg</td>\n",
              "      <td>bharatanatyam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Image         target\n",
              "0  508.jpg         odissi\n",
              "1  246.jpg   mohiniyattam\n",
              "2  473.jpg         odissi\n",
              "3  485.jpg         odissi\n",
              "4  128.jpg  bharatanatyam"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxDoMLlw0PlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv('/content/drive/My Drive/dataset/vgg19.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adwOADIpGt7D",
        "colab_type": "text"
      },
      "source": [
        "### Combine above 3 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm09gm6fGqiD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "2a862e0a-8865-4bb3-8181-e0b1c9803091"
      },
      "source": [
        "labels=0.8*label3+0.1*label2+0.1*label1\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0000000e+00, 1.0000000e-01, 8.0000001e-01, ..., 1.7520537e-28,\n",
              "        2.2553354e-29, 1.0000000e-01],\n",
              "       [0.0000000e+00, 8.9824152e-01, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        1.7584724e-03, 1.0000000e-01],\n",
              "       [0.0000000e+00, 1.0000000e-01, 8.0000001e-01, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 1.0000000e-01],\n",
              "       ...,\n",
              "       [0.0000000e+00, 1.0000000e-01, 8.0000001e-01, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 1.0000000e-01],\n",
              "       [0.0000000e+00, 1.0000000e-01, 8.0000001e-01, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 1.0000000e-01],\n",
              "       [0.0000000e+00, 1.0000000e-01, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 1.0000000e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW-jy7v8HG6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = [np.argmax(i) for i in labels]\n",
        "class_label = [inverse_map[x] for x in label]\n",
        "submission = pd.DataFrame({ 'Image': test.Image, 'target': class_label })\n",
        "submission.head()\n",
        "submission.to_csv('/content/drive/My Drive/dataset/combine.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DOKC29GtuW6",
        "colab_type": "text"
      },
      "source": [
        "## Stacking of above 3 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgZlLf2FtuW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "40c0bdab-a69b-49aa-ff72-b7a11bf705eb"
      },
      "source": [
        "'''\n",
        "Input -> it is used to instantiate a Keras tensor.\n",
        "          A Keras tensor is a tensor object from the underlying backend (Theano or TensorFlow),\n",
        "          which we augment with certain attributes that allow us to build a Keras model just by \n",
        "          knowing the inputs and outputs of the model.\n",
        "Concatenate layer -> It takes as input a list of tensors, all of the same shape except for the concatenation axis,\n",
        "                     and returns a single tensor that is the concatenation of all inputs.\n",
        "concatenate -> Functional interface to the Concatenate layer.\n",
        "get_layer -> Retrieves a layer based on either its name (unique) or index\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-135-a98aacb9e5cc>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    ''''\u001b[0m\n\u001b[0m        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ya3LFWmVxK4C",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import concatenate\n",
        "def stacking_ensemble(members,input_shape,n_classes):\n",
        "  commonInput = Input(shape=input_shape)\n",
        "  out=[]\n",
        "\n",
        "  for model in members:\n",
        "    #model._name= model._name+\"test\"+ str(members.index(model)+1)\n",
        "    model._name= model.get_layer(index = 0)._name +\"-test\"+ str(members.index(model)+1)\n",
        "    out.append(model(commonInput))\n",
        "\n",
        "  modeltmp = concatenate(out,axis=-1)\n",
        "  modeltmp = Dense(32, activation='relu')(modeltmp)\n",
        "  modeltmp = Dense(16, activation='relu')(modeltmp)\n",
        "  modeltmp = Dense(n_classes, activation='softmax')(modeltmp)\n",
        "  stacked_model = Model(commonInput,modeltmp)\n",
        "  stacked_model.compile( loss='categorical_crossentropy',optimizer= 'adam', metrics=['accuracy'])\n",
        "\n",
        "  return stacked_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WiBQ2MNexQyZ",
        "colab": {}
      },
      "source": [
        "members=[model,model_2,model_3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LMMvAdo8xS8y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "fe1bd11b-e009-4cd6-e91a-484f5f888b6c"
      },
      "source": [
        "stacked_model= stacking_ensemble(members,(img_h,img_w,3),8)\n",
        "stacked_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inception_resnet_v2-test1 (Sequ (None, 8)            55199080    input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "resnet50-test2 (Sequential)     (None, 8)            24714248    input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "vgg19-test3 (Sequential)        (None, 8)            20353736    input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 24)           0           inception_resnet_v2-test1[1][0]  \n",
            "                                                                 resnet50-test2[1][0]             \n",
            "                                                                 vgg19-test3[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 32)           800         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 16)           528         dense_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 8)            136         dense_28[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 100,268,528\n",
            "Trainable params: 7,029,584\n",
            "Non-trainable params: 93,238,944\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4sIEV5RRxUmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58f306b0-001c-426a-bfe0-755971b713b4"
      },
      "source": [
        "stacked_model.fit(train_datagen.flow(x_train, to_categorical(y_train,8), batch_size=32),\n",
        "                    epochs=50,\n",
        "          callbacks=callbacks,\n",
        "          validation_data= valid_datagen.flow(x_valid, to_categorical(y_valid,8), batch_size=32),\n",
        "          verbose=1\n",
        "             )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "11/11 [==============================] - 17s 2s/step - loss: 2.0634 - accuracy: 0.1478 - val_loss: 2.0883 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "11/11 [==============================] - 5s 493ms/step - loss: 2.0027 - accuracy: 0.2841 - val_loss: 2.0271 - val_accuracy: 0.3158 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "11/11 [==============================] - 5s 492ms/step - loss: 1.9665 - accuracy: 0.3739 - val_loss: 2.0003 - val_accuracy: 0.3158 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "11/11 [==============================] - 5s 486ms/step - loss: 1.9015 - accuracy: 0.4522 - val_loss: 1.9396 - val_accuracy: 0.3158 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "11/11 [==============================] - 6s 506ms/step - loss: 1.8356 - accuracy: 0.4812 - val_loss: 1.8792 - val_accuracy: 0.4737 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "11/11 [==============================] - 5s 480ms/step - loss: 1.7517 - accuracy: 0.5594 - val_loss: 1.8187 - val_accuracy: 0.4737 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "11/11 [==============================] - 5s 478ms/step - loss: 1.6996 - accuracy: 0.5768 - val_loss: 1.7134 - val_accuracy: 0.5263 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "11/11 [==============================] - 5s 474ms/step - loss: 1.5851 - accuracy: 0.6290 - val_loss: 1.6358 - val_accuracy: 0.6316 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "11/11 [==============================] - 5s 489ms/step - loss: 1.5181 - accuracy: 0.6232 - val_loss: 1.5638 - val_accuracy: 0.5263 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "11/11 [==============================] - 5s 480ms/step - loss: 1.4562 - accuracy: 0.6696 - val_loss: 1.4815 - val_accuracy: 0.5789 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "11/11 [==============================] - 5s 482ms/step - loss: 1.3397 - accuracy: 0.6986 - val_loss: 1.2989 - val_accuracy: 0.6316 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "11/11 [==============================] - 5s 481ms/step - loss: 1.3015 - accuracy: 0.7130 - val_loss: 1.3307 - val_accuracy: 0.4737 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "11/11 [==============================] - 5s 484ms/step - loss: 1.2763 - accuracy: 0.6725 - val_loss: 1.4232 - val_accuracy: 0.6316 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "11/11 [==============================] - 5s 489ms/step - loss: 1.2918 - accuracy: 0.6377 - val_loss: 1.4112 - val_accuracy: 0.5789 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "11/11 [==============================] - 5s 485ms/step - loss: 1.2194 - accuracy: 0.6667 - val_loss: 1.3567 - val_accuracy: 0.5263 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "11/11 [==============================] - 5s 486ms/step - loss: 1.0322 - accuracy: 0.7507 - val_loss: 1.2813 - val_accuracy: 0.6842 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "11/11 [==============================] - 5s 499ms/step - loss: 0.9556 - accuracy: 0.7681 - val_loss: 1.2027 - val_accuracy: 0.6842 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "11/11 [==============================] - 5s 484ms/step - loss: 0.9714 - accuracy: 0.7681 - val_loss: 1.2914 - val_accuracy: 0.6316 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.0068 - accuracy: 0.6986\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "11/11 [==============================] - 5s 491ms/step - loss: 1.0068 - accuracy: 0.6986 - val_loss: 1.2272 - val_accuracy: 0.5789 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "11/11 [==============================] - 6s 501ms/step - loss: 1.0906 - accuracy: 0.6580 - val_loss: 1.1973 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
            "Epoch 21/50\n",
            "11/11 [==============================] - 5s 493ms/step - loss: 1.0509 - accuracy: 0.6696 - val_loss: 1.1533 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
            "Epoch 22/50\n",
            "11/11 [==============================] - 5s 490ms/step - loss: 0.9456 - accuracy: 0.7159 - val_loss: 1.1237 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
            "Epoch 23/50\n",
            "11/11 [==============================] - 5s 489ms/step - loss: 0.9658 - accuracy: 0.7072 - val_loss: 1.0729 - val_accuracy: 0.6842 - lr: 1.0000e-04\n",
            "Epoch 24/50\n",
            "11/11 [==============================] - 5s 483ms/step - loss: 0.9241 - accuracy: 0.7391 - val_loss: 1.0698 - val_accuracy: 0.6842 - lr: 1.0000e-04\n",
            "Epoch 25/50\n",
            "11/11 [==============================] - 5s 495ms/step - loss: 0.8313 - accuracy: 0.8058 - val_loss: 0.9916 - val_accuracy: 0.7895 - lr: 1.0000e-04\n",
            "Epoch 26/50\n",
            "11/11 [==============================] - 5s 495ms/step - loss: 0.8603 - accuracy: 0.7710 - val_loss: 1.0023 - val_accuracy: 0.6842 - lr: 1.0000e-04\n",
            "Epoch 27/50\n",
            "11/11 [==============================] - 5s 480ms/step - loss: 0.7457 - accuracy: 0.8203 - val_loss: 0.9786 - val_accuracy: 0.7368 - lr: 1.0000e-04\n",
            "Epoch 28/50\n",
            "11/11 [==============================] - 5s 498ms/step - loss: 0.7852 - accuracy: 0.8029 - val_loss: 0.9302 - val_accuracy: 0.7368 - lr: 1.0000e-04\n",
            "Epoch 29/50\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7594 - accuracy: 0.8232\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "11/11 [==============================] - 5s 489ms/step - loss: 0.7594 - accuracy: 0.8232 - val_loss: 1.0316 - val_accuracy: 0.7368 - lr: 1.0000e-04\n",
            "Epoch 30/50\n",
            "11/11 [==============================] - 6s 509ms/step - loss: 0.7513 - accuracy: 0.8261 - val_loss: 1.0473 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 31/50\n",
            "11/11 [==============================] - 5s 498ms/step - loss: 0.7055 - accuracy: 0.8232 - val_loss: 1.0528 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 32/50\n",
            "11/11 [==============================] - 5s 488ms/step - loss: 0.7002 - accuracy: 0.8232 - val_loss: 1.0551 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 33/50\n",
            "11/11 [==============================] - 5s 488ms/step - loss: 0.7356 - accuracy: 0.8290 - val_loss: 1.0542 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 34/50\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7416 - accuracy: 0.8290\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "11/11 [==============================] - 5s 487ms/step - loss: 0.7416 - accuracy: 0.8290 - val_loss: 1.0532 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 35/50\n",
            "11/11 [==============================] - 5s 490ms/step - loss: 0.7094 - accuracy: 0.8406 - val_loss: 1.0539 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 36/50\n",
            "11/11 [==============================] - 5s 493ms/step - loss: 0.7302 - accuracy: 0.8493 - val_loss: 1.0535 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 37/50\n",
            "11/11 [==============================] - 5s 489ms/step - loss: 0.6995 - accuracy: 0.8551 - val_loss: 1.0532 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 38/50\n",
            "11/11 [==============================] - 5s 489ms/step - loss: 0.7053 - accuracy: 0.8493 - val_loss: 1.0521 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 39/50\n",
            "11/11 [==============================] - 5s 487ms/step - loss: 0.6812 - accuracy: 0.8377 - val_loss: 1.0499 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 40/50\n",
            "11/11 [==============================] - 5s 488ms/step - loss: 0.7177 - accuracy: 0.8435 - val_loss: 1.0487 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 41/50\n",
            "11/11 [==============================] - 5s 494ms/step - loss: 0.7119 - accuracy: 0.8435 - val_loss: 1.0479 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 42/50\n",
            "11/11 [==============================] - 5s 492ms/step - loss: 0.6632 - accuracy: 0.8551 - val_loss: 1.0475 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 43/50\n",
            "11/11 [==============================] - 5s 481ms/step - loss: 0.6626 - accuracy: 0.8638 - val_loss: 1.0466 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 44/50\n",
            "11/11 [==============================] - 5s 481ms/step - loss: 0.7169 - accuracy: 0.8319 - val_loss: 1.0468 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 45/50\n",
            "11/11 [==============================] - 5s 489ms/step - loss: 0.6939 - accuracy: 0.8435 - val_loss: 1.0474 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 46/50\n",
            "11/11 [==============================] - 5s 499ms/step - loss: 0.6434 - accuracy: 0.8783 - val_loss: 1.0458 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 47/50\n",
            "11/11 [==============================] - 6s 501ms/step - loss: 0.6465 - accuracy: 0.8638 - val_loss: 1.0448 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 48/50\n",
            "11/11 [==============================] - 5s 495ms/step - loss: 0.7168 - accuracy: 0.8377 - val_loss: 1.0451 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 49/50\n",
            "11/11 [==============================] - 5s 491ms/step - loss: 0.7031 - accuracy: 0.8522 - val_loss: 1.0457 - val_accuracy: 0.6842 - lr: 1.0000e-05\n",
            "Epoch 50/50\n",
            "11/11 [==============================] - 5s 499ms/step - loss: 0.6656 - accuracy: 0.8551 - val_loss: 1.0471 - val_accuracy: 0.6842 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f86d9d7e400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XSuBMOQmxXOo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "49689406-2498-4008-af1e-dece9fb9ffe1"
      },
      "source": [
        "labels = stacked_model.predict(test_img)\n",
        "label = [np.argmax(i) for i in labels]\n",
        "class_label = [inverse_map[x] for x in label]\n",
        "submission = pd.DataFrame({ 'Image': test.Image, 'target': class_label })\n",
        "submission.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>508.jpg</td>\n",
              "      <td>kuchipudi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>246.jpg</td>\n",
              "      <td>kathakali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>473.jpg</td>\n",
              "      <td>kathakali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>485.jpg</td>\n",
              "      <td>kuchipudi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128.jpg</td>\n",
              "      <td>bharatanatyam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>410.jpg</td>\n",
              "      <td>kuchipudi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>465.jpg</td>\n",
              "      <td>kathakali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>196.jpg</td>\n",
              "      <td>kathakali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>340.jpg</td>\n",
              "      <td>manipuri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>467.jpg</td>\n",
              "      <td>kathakali</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Image         target\n",
              "0  508.jpg      kuchipudi\n",
              "1  246.jpg      kathakali\n",
              "2  473.jpg      kathakali\n",
              "3  485.jpg      kuchipudi\n",
              "4  128.jpg  bharatanatyam\n",
              "5  410.jpg      kuchipudi\n",
              "6  465.jpg      kathakali\n",
              "7  196.jpg      kathakali\n",
              "8  340.jpg       manipuri\n",
              "9  467.jpg      kathakali"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eCIIb5l_xcrA",
        "colab": {}
      },
      "source": [
        " submission.to_csv('/content/drive/My Drive/dataset/stacking.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}